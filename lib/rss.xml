<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[ML-Course]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>ML-Course</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Fri, 08 Nov 2024 19:42:13 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Fri, 08 Nov 2024 19:42:04 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[Uzupełnianie brakujących wartości]]></title><description><![CDATA[ 
 <br><br>Jeżeli brakuje danych - dana wartość może być pusta, ale może być wstawiona jakaś wartość domyślna (np. w kolumnie wzrost mogłyby być wpisane wartości 0, które od razu widać, że nie mają sensu) - można podjąć następujące kroki:<br>
<br>poddać się (to nie w naszym stylu),
<br>usunąć kolumnę - warto zrobić jeżeli w danej kolumnie brakuje wielu danych, których nie ma jak uzupełnić lub są to dane, które nie są użyteczne,
<br># Przykładowe dane z brakującymi wartościami
data = {'A': [1, 2, None, 4, 5],
        'B': [None, 2, 3, None, 5],
        'C': [1, None, None, 4, 5]}

# Tworzenie DataFrame
df = pd.DataFrame(data)

df_without_cols = df.drop(columns=['column_name_1', 'column_name_2'])
<br>
<br>uzupełnić kolumnę przy pomocy określonej wartości (np. średniej wartości w danej kolumnie, najczęściej występującej wartości w danej kolumnie lub mediany wartości tej kolumny) lub wykorzystać regresję, aby na podstawie danych z innych kolumn uzupełnić te brakujące miejsca.
<br>from sklearn.impute import SimpleImputer

# Wykorzystamy DataFrame z poprzedniego fragmentu kodu
# Wypełnianie wartością średnią
imputer_mean = SimpleImputer(strategy='mean')
df_imputed_mean = pd.DataFrame(imputer_mean.fit_transform(df), columns=df.columns)

#Wypełnianie medianą
imputer_median = SimpleImputer(strategy='median')
df_imputed_median = pd.DataFrame(imputer_median.fit_transform(df), columns=df.columns)

# Wypełnianie najczęstszą wartością
imputer_most_frequent = SimpleImputer(strategy='most_frequent')
df_imputed_most_frequent = pd.DataFrame(imputer_most_frequent.fit_transform(df), columns=df.columns)

# Wypełnianie stałą wartością podaną przez użytkownika
imputer_constant = SimpleImputer(strategy='constant', fill_value=5)
df_imputed_constant = pd.DataFrame(imputer_constant.fit_transform(df), columns=df.columns)

# Metody korzystające z regresji
from sklearn.experimental import enable_iterative_imputer 
from sklearn.impute import IterativeImputer, KNNImputer

# Wypenianie przy pomocy regresji wielokrotnej
iter_imputer = IterativeImputer()
df_iter_imputed = pd.DataFrame(iter_imputer.fit_transform(df), columns=df.columns)

# Wypełnianie za pomocą k-Najbliższych sąsiadów
knn_imputer = KNNImputer(n_neighbors=3)
df_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)
<br>
Należy pamiętać, że po podzieleniu datasetu na zbiór treningowy, walidacyjny i testowy należy wykorzystywać ten sam imputer i skalery dla wszystkich zbiorów. Oznacza to, że należy wykorzystać fit lub fit_transform przy zbiorze treningowym, a dla pozostałych wykorzystywać samo transform.
<br><br>Przejdź do kolejnego zagadnienia (<a data-href="Normalizacja i standaryzacja" href="tematy\files-przygotowanie-danych-i-inżynieria-cech\normalizacja-i-standaryzacja.html" class="internal-link" target="_self" rel="noopener nofollow">Normalizacja i standaryzacja</a>) lub kliknij <a data-tooltip-position="top" aria-label="Przygotowanie danych i inżynieria cech" data-href="Przygotowanie danych i inżynieria cech" href="tematy\przygotowanie-danych-i-inżynieria-cech.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.]]></description><link>tematy\files-przygotowanie-danych-i-inżynieria-cech\brakujące-dane.html</link><guid isPermaLink="false">Tematy/Files Przygotowanie danych i inżynieria cech/Brakujące dane.md</guid><pubDate>Wed, 30 Oct 2024 22:31:48 GMT</pubDate></item><item><title><![CDATA[Sprawdzenie danych]]></title><description><![CDATA[ 
 <br><br>Pierwszym co należy zrobić przed przystąpieniem do uczenia implementacji algorytmów uczenia maszynowego jest przejrzenie datasetu. Korzystając z poznanych wcześniej bibliotek możemy użyć następujących komend:<br>
<br>sprawdzenie liczby brakujących wartości w poszczególnych kolumnach:
<br>print(pandas_dataframe.isnull().sum())
<br>
<br>obliczenie korelacji pomiędzy poszczególnymi kolumnami i wyświetlenie ich w formie macierzy korelacji:
<br># Wyliczenie macierzy korelacji
corr = pandas_dataframe.corr()

# Przygotowanie wykresu i wyświetlenie go (wymaga importowania seaborn i matplotlib)
plt.figure()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")

plt.title('Correlation Matrix')
plt.show()
<br>
<br>sprawdzić jakie unikalne wartości przyjmują dane w określonej kolumnie:
<br>pandas_dataframe['column_name'].unique()
<br>
<br>wyświetlić dane numeryczne w formie histogramu:
<br>pandas_dataframe['column_name'].plot.hist(bins=5, color='red', edgecolor='black')
<br><br>Wartości odstające, zwane również outliers, to wartości, które znacznie różnią się od reszty danych. W rezultacie mogą one niekorzystnie wpływać na wyniki analizy danych i modele uczenia maszynowego poprzez:<br>
<br>znaczne wpływanie na średnią, wariancję i inne statystyki opisowe,
<br>wprowadzanie szumu do modeli,
<br>prowadzenie do błędnych wniosków z analizy danych.
<br>W jaki sposób można sobie poradzić, jeżeli w danych występują wartości odstające:<br>
<br>usunięcie outlierów (np. jeżeli wynikają z błędu pomiaru lub wpisania danych i nie da się ich przybliżyć do reszty danych),
<br>winsoryzacja - zamiana wartości odstających na wartości graniczne pewnego przedziału (np. na wartości 95 percentyla),
<br>zamiana wartości odstających medianą (dzięki temu mniej wpływają na rozkład),
<br>wykorzystać normalizację, standaryzację lub podział danych na przedziały (o tym na dalszych podstronach).
<br><br>Najprostszą metodą identyfikacji wartości odstających jest użycie reguły rozstępu międzykwartylowego (IQR)<a data-footref="1" href="about:blank#fn-1-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a>.<br>
<br>IQR (Interquartile Range): Różnica między trzecim kwartylem (75 percentyl) a pierwszym kwartylem (25 percentyl).
<br>Outliery są wartościami, które są poniżej  lub powyżej .
<br>Implementacja identyfikacji wartości odstających i winsoryzacji oraz filtrowania znajduje się poniżej:<br>import pandas as pd
import numpy as np

# Utworzenie dataframe
data = {'Wartość': [10, 12, 14, 15, 100, 13, 14, 16, 12, 13, 11, 110]}
df = pd.DataFrame(data)

# 1. Obliczenie Q1 (pierwszego kwartylu) i Q3 (trzeciego kwartylu)
Q1 = df['Wartość'].quantile(0.25)
Q3 = df['Wartość'].quantile(0.75)
IQR = Q3 - Q1  # Rozstęp międzykwartylowy

# 2. Definiowanie granic dla wartości odstających
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# 3. Usunięcie danych odstających
df_no_outliers = df[(df['Wartość'] &gt;= lower_bound) &amp; (df['Wartość'] &lt;= 
		upper_bound)]

# 4. Winsoryzacja - zastępowanie outlierów wartościami granicznymi 
df_winsorized = df.copy() 
df_winsorized['Wartość'] = np.where(df_winsorized['Wartość'] &gt; upper_bound, 
		upper_bound, np.where(df_winsorized['Wartość'] &lt; lower_bound, 
		lower_bound, df_winsorized['Wartość']))
<br><br>Przejdź do kolejnego zagadnienia (<a data-href="Brakujące dane" href="tematy\files-przygotowanie-danych-i-inżynieria-cech\brakujące-dane.html" class="internal-link" target="_self" rel="noopener nofollow">Brakujące dane</a>) lub kliknij <a data-tooltip-position="top" aria-label="Przygotowanie danych i inżynieria cech" data-href="Przygotowanie danych i inżynieria cech" href="tematy\przygotowanie-danych-i-inżynieria-cech.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.<br><br><br>
<br>
<br><a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Interquartile_range" target="_blank">https://en.wikipedia.org/wiki/Interquartile_range</a><a href="about:blank#fnref-1-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
]]></description><link>tematy\files-przygotowanie-danych-i-inżynieria-cech\co-zrobić-po-otrzymaniu-zbioru-danych.html</link><guid isPermaLink="false">Tematy/Files Przygotowanie danych i inżynieria cech/Co zrobić po otrzymaniu zbioru danych.md</guid><pubDate>Sat, 12 Oct 2024 21:55:29 GMT</pubDate></item><item><title><![CDATA[Kodowanie kolumn]]></title><description><![CDATA[ 
 <br><br>W przypadku kolumn, w którym przechowywane są dane tekstowe, kluczowym krokiem jest zamiana tych wartości na wartości numeryczne przed użyciem w modelu uczenia maszynowego. W zależności od charakteru danych można zastosować różne metody:<br>
<br>kodowanie etykiet (label encoding) - polega na przypisaniu każdej unikalnej wartości tekstowej w kolumnie liczby całkowitej. Wadą może być jeżeli nie ma naturalnego porządku w kategoriach, bo model może zakładać, że wyższe liczby mają większe znaczenie.
<br>from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['column_name'] = le.fit_transform(df['column_name']) 
#dla zbioru testowego/walidacyjnego należy enkodera utworzonego na zbiorze treningowym (w tym przypadku df):
test_df['column_name'] = le.transform(test_df['column_name'])
<br>
<br>one-hot encoding: tworzy osobne kolumny dla każdej unikalnej wartości tekstowej w kolumnie, a następnie przypisuje wartość 0 lub 1 w tych kolumnach, wskazując, czy dana obserwacja zawiera daną kategorię. Zaletą jest, że nie występuje wiele wartości, które mogłyby mylić model, ale wadą jest utworzenie wielu kolumn (co w przypadku rozbudowanych zbiorów danych może bardzo zwiększyć zapotrzebowanie na pamięć do przechowywania datasetu).
<br>df = pd.get_dummies(df, columns=['column_name'])
<br>
<br>mapowanie wartości: ręczne przypisane liczby do wartości tekstowych, co bywa przydatne w przypadku danych z wyraźnym porządkiem logicznym.
<br>mapping = {'val_1': 1, 'val_2': 2, 'val_3': 3} 
df['column_name'] = df['column_name'].map(mapping)
<br>Przykład jak wygląda zakodowanie kolumny ocena, w której przechowywane są wartości: niska, średnia i wysoka:<br>
<br>Podstawowa kolumna:
<br><br>
<br>Zakodowana kolumna przy pomocy label encoding (wysoka -&gt; 0, niska -&gt; 1, średnia -&gt; 2)
<br><br>
<br>Wykorzystanie one-hot encoding:
<br><br>
<br>Wykorzystanie mapowania {'niska': 0, 'średnia': 1, 'wysoka': 2}:
<br><br><br>Data binning jest techniką wstępnej obróbki danych, która polega na grupowaniu ciągłych danych liczbowych w dyskretne przedziały lub "koszyki". W efekcie, zamiast mieć nieskończony zestaw możliwych wartości liczbowych, przekształcamy dane na określoną liczbę przedziałów, co ułatwia analizę i może poprawić działanie niektórych modeli uczenia maszynowego.<br>Zaletami jest uproszczenie interpretacji (na danych ciągłych może być trudność w analizie datasetu), obsługę nieliniowości, zmniejszenie wpływu szumu (po podziale na koszyki drobne fluktuacje mają mniejsze znaczenie, co pozwala zrozumieć ogólne trendy), oraz ułatwienie modelowania, bo część modeli (np. drzewa decyzyjne) mogą działać lepiej, kiedy dane zamiast być ciągłe to są podzielone na przedziały.<br>Wyróżnić można następujące rodzaje binningu:<br>
<br>podział na przedziały o równej szerokości (każdy przedział ma taką samą szerokość, ale liczba elementów w każdym koszyku może się różnić),
<br>podział na przedziały o równej liczebności (każdy przedział ma taką samą liczbę obserwacji, ale szerokość każdego przedziału zależy od rozkładu danych),
<br>podział na przedziały ze względu na dane i ich zastosowanie (np. kolumna wiek mogłaby zostać podzielona na 3 podzbiory 0-18, 18-65 i 65+).
<br>Przykład implementacji każdej z przedstawionych metod znajduje się poniżej:<br>import pandas as pd

# Przykładowy DataFrame z kolumną 'Wiek'
data_wiek = {'Wiek': [5, 17, 25, 45, 67, 72, 15, 60, 19, 84]}
df_wiek = pd.DataFrame(data_wiek)

# Przedziały o równej szerokości
df_wiek['Wiek_equal_width'] = pd.cut(df_wiek['Wiek'], bins=3, labels=[0, 1, 2])

# Przedziały o równej liczebności
df_wiek['Wiek_equal_freq'] = pd.qcut(df_wiek['Wiek'], q=3, labels=[0, 1, 2])

# Na podstawie własnych reguł na koszyki 0-18, 18-65 i 65+
bins_business = [0, 18, 65, float('inf')]
labels_business = [0, 1, 2]
df_wiek['Wiek_own_rules'] = pd.cut(df_wiek['Wiek'], bins=bins_business, labels=labels_business)
<br><br>Czasami w procesie analizy danych nie wystarczy po prostu użyć surowych wartości — warto pomyśleć o tym, jak możemy z istniejących danych wyciągnąć dodatkowe informacje lub wyliczyć bardziej użyteczne cechy, które mogą mieć większe znaczenie dla modelu lub analizy. Tego rodzaju proces, zwany feature engineering, może znacznie poprawić wyniki modelu i jakość wniosków. Oto kilka przykładów, jak można wykorzystać istniejące dane do wyciągania dodatkowych informacji:<br>
<br>wyliczenie wartości średniej: np. posiadając liczbę transakcji sklepu w ciągu miesiąca i jaka była suma transakcji w miesiącu można obliczyć średnią, co może mieć znaczenie przy podawaniu danych do modelu,
<br>obliczenie procentów/proporcji: np. wyliczenie jaki procent transakcji sklepu to zakupy pieczywa,
<br>sumowanie wartości z kilku kolumn,
<br>wyciąganie dodatkowych informacji, np. kolumna przechowująca wartości binarne, które powstają przez sprawdzenie warunku "Czy dany sklep ma minimum 15 transakcji w miesiącu i średnią wartość transakcji powyżej 50pln?",
<br>wyciąganie cech z kolumn tekstowych, np. długość tekstu, ilość słów, ilość hasztagów, występowanie poszczególnych słów, etc.
<br><br>Kliknij <a data-tooltip-position="top" aria-label="Przygotowanie danych i inżynieria cech" data-href="Przygotowanie danych i inżynieria cech" href="tematy\przygotowanie-danych-i-inżynieria-cech.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu. Możesz stamtąd przejść do wykonania zadań przygotowanych w Jupyter Notebook.]]></description><link>tematy\files-przygotowanie-danych-i-inżynieria-cech\inżynieria-cech.html</link><guid isPermaLink="false">Tematy/Files Przygotowanie danych i inżynieria cech/Inżynieria cech.md</guid><pubDate>Sat, 12 Oct 2024 21:25:26 GMT</pubDate></item><item><title><![CDATA[Normalizacja]]></title><description><![CDATA[ 
 <br><br>Normalizacja polega na przekształceniu danych tak, aby mieściły się w określonym zakresie (zwykle od 0 do 1). Jest to przydatne, gdy dane mają bardzo różne skale, a chcemy by na wejściu algorytmu wartości mieściły się w określonym przedziale.<br>Wzór na normalizację:<br><br>gdzie:<br>
<br> to oryginalna wartość,
<br>​ to minimalna wartość w danym zbiorze,
<br> to maksymalna wartość w danym zbiorze.
<br>Przeprowadzenie normalizacji w języku Python możliwe jest przy pomocy biblioteki Scikit-Learn<br>from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_normalized = scaler.fit_transform(df)  # Normalizacja całego DataFrame
df_normalized = pd.DataFrame(df_normalized, columns=df.columns)

<br><br>Standaryzacja polega na przekształceniu danych tak, aby miały średnią równą 0 i odchylenie standardowe równe 1. Jest to przydatne, gdy chcemy, aby różne cechy miały porównywalny wpływ na model (niektóre algorytmy jak regresja liniowa czy SVM są bardzo wrażliwe na skalę danych). <br>Formuła standaryzacji:<br><br>gdzie:<br>
<br> to oryginalna wartość,
<br> to średnia wartość cechy,
<br> to odchylenie standardowe cechy (pierwiastek wariancji).
<br>Zastosowanie standaryzacji w języku Python możliwe jest przy pomocy biblioteki Scikit-Learn<br>from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_standardized = scaler.fit_transform(df)  # Standaryzacja całego DataFrame
df_standardized = pd.DataFrame(df_standardized, columns=df.columns)
<br>
Należy pamiętać, że po podzieleniu datasetu na zbiór treningowy, walidacyjny i testowy należy wykorzystywać ten sam imputer i skalery dla wszystkich zbiorów. Oznacza to, że należy wykorzystać fit lub fit_transform przy zbiorze treningowym, a dla pozostałych wykorzystywać samo transform.
<br><br>Przejdź do kolejnego zagadnienia (<a data-href="Inżynieria cech" href="tematy\files-przygotowanie-danych-i-inżynieria-cech\inżynieria-cech.html" class="internal-link" target="_self" rel="noopener nofollow">Inżynieria cech</a>) lub kliknij <a data-tooltip-position="top" aria-label="Przygotowanie danych i inżynieria cech" data-href="Przygotowanie danych i inżynieria cech" href="tematy\przygotowanie-danych-i-inżynieria-cech.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.]]></description><link>tematy\files-przygotowanie-danych-i-inżynieria-cech\normalizacja-i-standaryzacja.html</link><guid isPermaLink="false">Tematy/Files Przygotowanie danych i inżynieria cech/Normalizacja i standaryzacja.md</guid><pubDate>Sat, 12 Oct 2024 22:03:22 GMT</pubDate></item><item><title><![CDATA[Jedna biblioteka praktycznie do wszystkiego]]></title><description><![CDATA[ 
 <br><br>Scikit-learn jest jedną z najczęściej wybieranych bibliotek do projektów uczenia maszynowego, zarówno przez początkujących, jak i zaawansowanych użytkowników, co zawdzięcza swojej elastyczności, wszechstronności i prostocie użycia. Zapewnia narzędzia do budowy i trenowania modeli, a także do analizy danych i zapewnia kompatybilność z innymi narzędziami do analizy danych dostępnych w Pythonie. <br>Link do oficjalnej strony <a data-tooltip-position="top" aria-label="https://scikit-learn.org/" rel="noopener nofollow" class="external-link" href="https://scikit-learn.org/" target="_blank">scikit-learn</a>.<br><img alt="scikit-learn-logo.png" src="lib\media\scikit-learn-logo.png"><br>
Źródło: <a data-footref="sci" href="about:blank#fn-1-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a><br><br>Bibliotekę sklearn możemy wykorzystać do:<br>
<br>pobierania datasetów, przetwarzania danych i ich wstępnej obróbki,
<br>implementacji algorytmów uczenia maszynowego:

<br>klasyfikacji (np. Decision Tree, Random Forest, Gradient Boosting),
<br>regresji (np. Linear Regression, Ridge Regression),
<br>klasteryzacji (np. K-means, DBSCAN),


<br>optymalizacja hiperparametrów (przy pomocy Grid Search lub Random Search) i walidacja modeli (zaimplementowane są różne metryki jak cross-validation, F1, accuracy)
<br>tworzenie pipeline'ów (połączenie kilku kroków przetwarzania), aby pomóc w lepszym zorganizowaniu budowania modeli i ich walidacji.
<br><br>W scikit-learn dostępne jest kilka funkcji, które mogą być wykorzystane przy różnych modelach i procesach uczenia maszynowego, co ułatwia korzystanie z tej biblioteki. Poniżej znajduje się kilka przykładów:<br>
<br>podział danych na zbiór treningowy i testowy (jako X oznaczone są cechy, a jako y etykiety)
<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
<br>
<br>trening modelu na danych X_train i etykietach y_train:
<br>model.fit(X_train, y_train)
<br>
<br>przewidywanie etykiet dla nowych danych X_test:
<br>model.predict(X_test)
<br>
<br>obliczenie oceny modelu (np. dla klasyfikacji oblicza dokładność, a dla regresji błąd):
<br>model.score(X_test, y_test)
<br>
<br>jednoczesne dopasowanie do danych i transformacja (np. przy skalowaniu lub uzupełnianiu brakujących wartości):
<br>scaler.fit_transform(training_data)
<br>
<br>sama transformacja (np. dla danych testowych, gdzie chcemy wykorzystać to samo skalowanie co dla danych treningowych):
<br>scaler.transform(test_data)
<br>
<br>przeprowadzenie walidacji krzyżowej:
<br>cross_val_score(model, X_train, y_train, cv=5)
<br>
<br>optymalizacja hiperparametrów:
<br># param_grid jest słownikiem i dla drzewa decyzyjnego może wyglądać następująco:
param_grid = {'max_depth': [None, 10, 20, 30], 
			  'min_samples_split': [2, 10, 20], 
			  'min_samples_leaf': [1, 5, 10], 
			  'max_features': [None, 'sqrt', 'log2']}
# a przeszukiwanie przy pomocy przeszukiwania po siatce hiperparemetrów
GridSearchCV(model(), param_grid, cv=5)
<br>
<br>wczytywanie danych:
<br>from sklearn import datasets
# Wczytanie datasetu Iris
iris = datasets.load_iris()

# Dostępne są inne datasety, do których dostęp jest za pomocą datasets.load_nazwa(), np.
digits = datasets.load_digits()
wine = datasets.load_wine()

# Możliwe jest też pobieranie datasetów z OpenML, np.
mnist = datasets.fetch_openml('mnist_784', version=1)
<br><br>Utworzenie samemu zbioru danych jest czasochłonne i wymaga następnie przetworzenia danych, co ponownie może być czasochłonne i dodatkowo skomplikowane. Istnieją strony, na których użytkownicy mogą wstawić datasety, aby inni mogli z nich publicznie korzystać. Najpopularniejsze z nich to:<br>
<br><a data-tooltip-position="top" aria-label="https://www.kaggle.com/datasets" rel="noopener nofollow" class="external-link" href="https://www.kaggle.com/datasets" target="_blank">Kaggle</a>
<br><a data-tooltip-position="top" aria-label="https://openml.org/search?type=data" rel="noopener nofollow" class="external-link" href="https://openml.org/search?type=data" target="_blank">OpenML</a><br>
Ponadto na Kaggle odbywają się cyklicznie konkursy (z dużymi pulami nagród), w których można spróbować swoich sił.
<br><br>Przejdź do kolejnego zagadnienia (<a data-href="Implementacja regresji przy pomocy Scikit-learn" href="tematy\files-regresja-liniowa-(i-nie-tylko)\implementacja-regresji-przy-pomocy-scikit-learn.html" class="internal-link" target="_self" rel="noopener nofollow">Implementacja regresji przy pomocy Scikit-learn</a>) lub kliknij <a data-tooltip-position="top" aria-label="Regresja liniowa (i nie tylko)" data-href="Regresja liniowa (i nie tylko)" href="tematy\regresja-liniowa-(i-nie-tylko).html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.<br><br><br>
<br>
<br><a rel="noopener nofollow" class="external-link" href="https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos" target="_blank">https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos</a><a href="about:blank#fnref-1-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
]]></description><link>tematy\files-regresja-liniowa-(i-nie-tylko)\czym-jest-scikit-learn.html</link><guid isPermaLink="false">Tematy/Files Regresja liniowa (i nie tylko)/Czym jest Scikit-learn.md</guid><pubDate>Mon, 14 Oct 2024 19:15:45 GMT</pubDate><enclosure url="lib\media\scikit-learn-logo.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\scikit-learn-logo.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Jak zaimplementować regresję w Pythonie?]]></title><description><![CDATA[ 
 <br><br><br># Import niezbędnych bibliotek
import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
<br><br>X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
<br><br># Podział danych na zestaw treningowy i testowy
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
<br><br># Regresja liniowa
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# Drzewo decyzyjne
model_dt = DecisionTreeRegressor()
model_dt.fit(X_train, y_train)

# Las losowy
model_rf = RandomForestRegressor(n_estimators=100)
model_rf.fit(X_train, y_train)

# SVR
model_svr = SVR()
model_svr.fit(X_train, y_train)
<br><br>y_pred_lr = model.predict(X_test)
y_pred_dt = model.predict(X_test)
y_pred_rf = model.predict(X_test)
y_pred_svr = model.predict(X_test)
<br><br>mse_lr = mean_squared_error(y_test, y_pred_lr)
mse_dt = mean_squared_error(y_test, y_pred_dt)
mse_rf = mean_squared_error(y_test, y_pred_rf)
mse_svr = mean_squared_error(y_test, y_pred_svr)
<br><br>import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline

X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tworzenie Pipeline dla modelu SVR
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Skalowanie danych
    ('svr', SVR())                 # Regresor SVR
])

# Trenowanie Pipeline
pipeline.fit(X_train, y_train)

# Predykcja przy użyciu Pipeline
y_pred_pipeline = pipeline.predict(X_test)

# Ocena modelu Pipeline
mse_pipeline = mean_squared_error(y_test, y_pred_pipeline)
<br><br>Kliknij <a data-tooltip-position="top" aria-label="Regresja liniowa (i nie tylko)" data-href="Regresja liniowa (i nie tylko)" href="tematy\regresja-liniowa-(i-nie-tylko).html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu. Możesz stamtąd przejść do wykonania zadań przygotowanych w Jupyter Notebook.]]></description><link>tematy\files-regresja-liniowa-(i-nie-tylko)\implementacja-regresji-przy-pomocy-scikit-learn.html</link><guid isPermaLink="false">Tematy/Files Regresja liniowa (i nie tylko)/Implementacja regresji przy pomocy Scikit-learn.md</guid><pubDate>Mon, 14 Oct 2024 20:44:55 GMT</pubDate></item><item><title><![CDATA[Rodzaje regresji]]></title><description><![CDATA[ 
 <br><br>Istnieją różne rodzaje regresji, które różnią się rodzajem zależności między zmiennymi, które modelują. Najczęściej występujące to:<br>
<br>Regresja liniowa: Podstawowy rodzaj regresji, w którym założeniem jest liniowa zależność zmiennych (czyli można ją przedstawić za pomocą prostej), np. droga i czas w ruchu jednostajnym prostoliniowym,
<br>Regresja wielomianowa: Służy do modelowania bardziej złożonych zależności, których nie można opisać przy pomocy prostej, np. wzrost człowieka i wiek,
<br>Regresja logistyczna: Służy do przewidywania wartości binarnych (tak/nie, 0/1), np. czy stwierdzenie czy uda się przejechać drogę o danej długości bez tankowania,
<br>Regresja drzewiasta: Buduje model w postaci drzewa decyzyjnego, co pozwala na interpretację wyników, np. predykcja ceny domu, gdzie każdy węzeł jest pytaniem o jedną z cech (metraż, wiek, etc.).
<br>Aby łatwiej było zrozumieć różnice, poniżej znajdują się przykładowe wykresy z wrysowanymi poszczególnymi typami regresji<br><img alt="regression_types.png" src="lib\media\regression_types.png"><br>A na grafice poniżej widoczna jest wizualizacja drzewa decyzyjnego z przykładu powyżej, gdzie widać jak parametru x wpływa na wartość przewidywaną przez drzewo. Taki "schodkowy" przebieg regresji wynika z głębokości drzewa równej zaledwie max_depth=3, więc max 8 wartości na wyjściu.<br><img alt="decission_tree_regression_plot.png" src="lib\media\decission_tree_regression_plot.png"><br><br><br>Drzewo decyzyjne, to taki algorytm, który dzieli dane na mniejsze grupy na podstawie warunków logicznych. Model ma strukturę drzewa, w którym każdy węzeł reprezentuje kolejne pytania, gałęzie prowadzą do odpowiedzi, a w liściach znajdują się predykcje.<br>Dobrym przykładem może być utworzenie drzewa do klasyfikacji zwierząt. Załóżmy, że chcemy mieć następujące klasy (na klasach jest łatwiej wytłumaczyć niż na regresji, ale różnica jest taka, że predykowana jest wartość, a nie klasa): kot, pies, sowa, orzeł, pająk, mrówka, wąż, dżdżownica. Drzewo decyzyjne mogłoby wyglądać następująco:<br><img alt="decision_tree_example.png" src="lib\media\decision_tree_example.png"><br>Zaletami drzew decyzyjnych jest możliwość wykorzystania zarówno cech numerycznych, jak i kategorycznych, oraz łatwość w interpretacji modelu. Wadą jednak jest podatność na przeuczenie (overfitting), które polega na zbyt mocnym dopasowaniu modelu do danych treningowych. Korzystając z rysunku powyżej, przeuczenie mogłoby się objawić tym, że pies, który przez jakiś czas badania nie zaszczekał, zostanie zakwalifikowany jako kot, bo w taki sposób zostało dobrane pytanie podczas treningu.<br><br>Random Forest polega na budowie wielu niezależnych drzew decyzyjnych, a następnie łączeniu ich wyników (poprzez uśrednianie dla regresji lub głosowanie większościowe dla klasyfikacji). Każde drzewo w lesie jest trenowane na losowym podzbiorze danych i atrybutów, co pomaga modelowi unikać przeuczenia. Inną zaletą jest to, że dobrze radzi sobie z dużymi zestawami danych, które mają dodatkowo dużo cech.<br><br>SVM to algorytm, który stara się znaleźć hiperpłaszczyznę w przestrzeni wielowymiarowej, która najlepiej oddzieli poszczególne klasy w danych. W przypadku problemów liniowych, SVM szuka płaszczyzny, która maksymalizuje odległość między najbliższymi punktami dwóch klas. Zaletą jest to, że SVM może korzystać z kerneli (jąder), które pozwalają na transformację danych do wyższych wymiarów, aby znalezienie płaszczyzny było możliwe dla problemów nieliniowych. Wadami jest jednak trudność w interpretacji oraz większa złożoność obliczeniowa.<br>Przykład jak algorytm SVM działa dla danych, w których występują 2 klasy, które oddzielić należy za pomocą okręgu, a nie liniowo pokazany jest poniżej. Poprzez wykorzystanie jądra RBF (Radial Basis Function) możliwe jest dodanie kolejnego wymiaru, w którym znaleziona zostaje hiperpłaszczyzna, która oddzieli dane na poszczególne klasy.<br><img alt="svm_rbf_2d_decision_boundary.png" src="lib\media\svm_rbf_2d_decision_boundary.png"><br>
<img alt="svm_rbf_3d_transformed_space.png" src="lib\media\svm_rbf_3d_transformed_space.png"><br><br>Przejdź do kolejnego zagadnienia (<a data-href="Czym jest Scikit-learn" href="tematy\files-regresja-liniowa-(i-nie-tylko)\czym-jest-scikit-learn.html" class="internal-link" target="_self" rel="noopener nofollow">Czym jest Scikit-learn</a>) lub kliknij <a data-tooltip-position="top" aria-label="Regresja liniowa (i nie tylko)" data-href="Regresja liniowa (i nie tylko)" href="tematy\regresja-liniowa-(i-nie-tylko).html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.]]></description><link>tematy\files-regresja-liniowa-(i-nie-tylko)\rodzaje-regresji.html</link><guid isPermaLink="false">Tematy/Files Regresja liniowa (i nie tylko)/Rodzaje regresji.md</guid><pubDate>Sun, 13 Oct 2024 21:00:58 GMT</pubDate><enclosure url="lib\media\regression_types.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\regression_types.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Funkcje w języku Python]]></title><description><![CDATA[ 
 <br><br>Funkcje w Pythonie umożliwiają podzielenie kodu na moduły, które można ponownie wykorzystywać. Tworzy się je za pomocą instrukcji def i ciała funkcji jak w poniższym przykładzie:<br>def nazwa_funkcji(argumenty):
	# tutaj co w funkcji się dzieje
	return wynik
<br>gdzie:<br>
<br>def - słowo kluczowe inicjujące definicję funkcji,
<br>nazwa_funkcji - unikalna nazwa funkcji,
<br>argumenty - opcjonalna lista argumentów, które funkcja przyjmuje (można opcjonalnie zdefiniować typ i wartość domyślną elementów),
<br>return - opcjonalne słowo kluczowe, zwracające wartość z funkcji;<br>
a wywołanie funkcji wygląda następująco:
<br># W przypadku funkcji zwracającej wartość
zmienna = nazwa_funkcji(argumenty)

# W przypadku funkcji, która nic nie zwraca (np. ma wyświetlić w konsoli odpowiedź, ale nie zwrócić wyniku)
nazwa_funkcji(argumenty)
<br>Przykłady tworzenia funkcji:<br>def oblicz_sume_kwadratow(x, y):
	result = x**2+y**2
	return result

# Przykład funkcji ze zdefiniowanymi typami danych dla argumentów i zwracanej wartości
def oblicz_kwadrat_sumy(x: float, y: float) -&gt;float:
	return (x+y)**2

# Przykład funkcji z wartością domyślną elementu
def oblicz_roznice_kwadratow(x, y=0.0):
	return x**2 - y**2

# Przykład funkcji z docstrings ("dokumentacją" funkcji)
def oblicz_kwadrat_roznicy(x, y):
  """Oblicza kwadrat różnicy dwóch liczb.

  Args:
    x: Pierwsza liczba.
    y: Druga liczba.

  Returns:
    Kwadrat różnicy liczb x i y.
  """
  return (x-y)**2
<br>Możliwe jest też zwracanie więcej niż 1 wartości, np:<br>import datetime

def pobierz_aktualna_date():
  """Zwraca aktualną datę w postaci roku, miesiąca i dnia."""
  dzisiaj = datetime.date.today()
  return dzisiaj.year, dzisiaj.month, dzisiaj.day

# Wywołanie funkcji i przypisanie zwróconych wartości do zmiennych
rok, miesiac, dzien = pobierz_aktualna_date()

print(f"Dzisiaj jest {dzien} {miesiac} {rok}.")
<br><br>Klasy są podstawowym elementem programowania obiektowego w Pythonie. Służą one do reprezentacji obiektów/pojęć w świecie rzeczywistym. Definicja klasy wygląda następująco:<br>class NazwaKlasy:
    def __init__(self, argumenty_init):
	    self.atrybuty_do_inicjalizacji = argumenty_init
	def metoda(self, argument_metody):
		self.atrybuty_do_modyfikacji = argumenty_metody
<br>gdzie:<br>
<br>class - słowo kluczowe inicjujące definicję klasy,
<br>NazwaKlasy - unikalna nazwa klasy,
<br>atrybuty - zmienne związane z obiektami tej klasy (np. kolor, rozmiar),
<br>metody - funkcje związane z obiektami tej klasy (np. poruszanie się, rysowanie).<br>
a utworzenie obiektu danej funkcji wygląda następująco:
<br>zmienna = NazwaKlasy(argumenty)
<br>Przykład tworzenia klasy:<br>class Pies:
    def __init__(self, imie, rasa):  
    # wewnątrz init inicjalizujemy wszystkie atrybuty klasy
        self.imie = imie
        self.rasa = rasa
        self.szczek = "Chał chał!"

	def zmien_imie(self, dzwiek):
		self.szczek = dzwiek

    def szczekaj(self):
        print(self.szczek)

moj_pies = Pies("Reksio", "Jamnik")
moj_pies.szczekaj()
# Dostęp do imienia psa
print(moj_pies.imie)
<br>Python nie ma mechanizmu ścisłej prywatności artybutów tak jak np. C++, ale wykorzystanie __ na początku nazwy pozwala na oznaczenie artybutu jako prywatny, do którego należy odwoływać się przez metodę danej klasy:<br>class Osoba:
    def __init__(self, imie, pesel):
        self.imie = imie
        self.__pesel = pesel  # Prywatny atrybut

    def pobierz_pesel(self):
        return self.__pesel
<br><img alt="PythonFruitClass.png" src="lib\media\pythonfruitclass.png"><br>
Źródło: <a data-footref="meme1" href="about:blank#fn-1-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a><br><br>Przejdź do kolejnego zagadnienia (<a data-href="Używanie bibliotek" href="tematy\files-wstęp-do-języka-python\używanie-bibliotek.html" class="internal-link" target="_self" rel="noopener nofollow">Używanie bibliotek</a>) lub kliknij <a data-tooltip-position="top" aria-label="Wstęp do języka Python" data-href="Wstęp do języka Python" href="tematy\wstęp-do-języka-python.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.<br><br><br>
<br>
<br><a rel="noopener nofollow" class="external-link" href="https://programmerhumor.io/python-memes/python-is-easy-they-say/" target="_blank">https://programmerhumor.io/python-memes/python-is-easy-they-say/</a><a href="about:blank#fnref-1-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
]]></description><link>tematy\files-wstęp-do-języka-python\funkcje-i-klasy.html</link><guid isPermaLink="false">Tematy/Files Wstęp do języka Python/Funkcje i klasy.md</guid><pubDate>Sun, 06 Oct 2024 20:42:32 GMT</pubDate><enclosure url="lib\media\pythonfruitclass.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pythonfruitclass.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Wgrywanie Notebooków]]></title><description><![CDATA[ 
 <br>W tym miejscu przedstawione zostaną 3 możliwości uruchomienia Jupyter Notebooks oraz krótki opis tego jak one działają.<br><br>Przed przejściem do wgrywania zacznijmy od pobierania Notebooków przygotowanych do tego kursu. Przejdź do repozytorium kursu, wejdź w katalog Tasks i wybierz, który plik chcesz wybrać (kliknij go dwukrotnie). Po przejściu do podglądu pliku, w górnej prawej części ekranu powinna być widoczna ikonka pobierania (Download raw file) i przy jej pomocy można pobrać plik. Należy pamiętać o tym, że do części zadań trzeba pobrać dodatkowo pliki z danymi (z folderu Dane).<br>
<img alt="Downloading_from_github.png" src="lib\media\downloading_from_github.png"><br>
Inną możliwością jest użycie git clone &lt;adres_url_repozytorium&gt;, który pobierze całe repozytorium z GitHuba i w ten sposób można przejść do folderu, który nas interesuje.<br><br><br><br><img alt="GoogleColab2.png" src="lib\media\googlecolab2.png"><br><br><img alt="GoogleColab1.png" src="lib\media\googlecolab1.png"><br><br><img alt="GoogleColab3.png" src="lib\media\googlecolab3.png"><br><br><img alt="GoogleColab4.png" src="lib\media\googlecolab4.png"><br><br><br><br><img alt="Kaggle1.png" src="lib\media\kaggle1.png"><br><br><img alt="Kaggle2.png" src="lib\media\kaggle2.png"><br><br><img alt="Kaggle3.png" src="lib\media\kaggle3.png"><br><br><img alt="Kaggle4.png" src="lib\media\kaggle4.png"><br><br><br>
<br>Visual Studio Code z oficjalnej strony: <a rel="noopener nofollow" class="external-link" href="https://code.visualstudio.com/" target="_blank">https://code.visualstudio.com/</a> 
<br>Pythona ze strony: <a rel="noopener nofollow" class="external-link" href="https://www.python.org/downloads/" target="_blank">https://www.python.org/downloads/</a> (w chwili tworzenia kursu polecam korzystać z Pythona 3.11)
<br><br><br><img alt="vscode1.png" src="lib\media\vscode1.png"><br><br>W przypadku, gdy nie mieliśmy wcześniej utworzonego środowiska:<br>
Python Environments &gt; Create Python Environment + Venv + Python 3.xx<br>
<img alt="vscode2.png" src="lib\media\vscode2.png"><br>
<img alt="vscode3.png" src="lib\media\vscode3.png"><br>
Jeżeli środowisko wcześniej utworzyliśmy to można zamiast Create Python Environment można je wybrać z listy<br>
<img alt="vscode4.png" src="lib\media\vscode4.png"><br><br><br>!pip install &lt;nazwa_biblioteki&gt;
<br>możesz też skorzystać z przygotowanego pliku requirements.txt<br>!pip install -r requirements.txt
<br><br>Na rysunku poniżej zaznaczono elementy widoczne w notatniku:<br> Komórka z tekstem 
 Komórka z kodem 
 Pole do wpisywania kodu 
 Przycisk do uruchomienia komórki z kodem 
 Wyjście komórki z kodem <br><img alt="notebook1.png" src="lib\media\notebook1.png"><br>Notebooki umożliwiają uruchamianie jedynie fragmentów kodu, przez co dobrze nadają się do realizacji ćwiczeń, które rozwiązuje się krok po kroku i gdzie przygotowuje się coś, co wykorzysta się później, dzięki zapamiętywaniu wartości zmiennych (są one resetowane przy rezetowaniu środowiska). Powoduje to, że nie ma konieczności wykonywania długich obliczeń za każdym uruchomieniem skryptu.<br><br>Kliknij <a data-tooltip-position="top" aria-label="Wstęp do języka Python" data-href="Wstęp do języka Python" href="tematy\wstęp-do-języka-python.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu. Możesz stamtąd przejść do wykonania zadań przygotowanych w Jupyter Notebook.]]></description><link>tematy\files-wstęp-do-języka-python\jak-korzystać-z-jupyter-notebooks.html</link><guid isPermaLink="false">Tematy/Files Wstęp do języka Python/Jak korzystać z Jupyter Notebooks.md</guid><pubDate>Mon, 04 Nov 2024 22:45:59 GMT</pubDate><enclosure url="lib\media\downloading_from_github.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\downloading_from_github.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Struktury danych w Python]]></title><description><![CDATA[ 
 <br><br>W Pythonie możemy wyróżnić następujące struktury danych: listy, krotki, zbiory i słowniki.<br>
<br>
Listy są najbardziej uniwersalnymi strukturami danych w Pythonie. Są to uporządkowane, mutowalne kolekcje, które mogą zawierać dowolne elementy. Możesz dodawać, usuwać i modyfikować elementy listy.

<br>
Tuple (krotki) są podobne do list, ale są niemutowalne, czyli po utworzeniu nie można zmieniać ich elementów. Są często używane do przechowywania danych, które nie powinny być modyfikowane.

<br>
Sety (zbiory) są nieuporządkowanymi zbiorami unikalnych elementów. Nie mogą zawierać duplikatów. Sety są używane do sprawdzania przynależności elementów, wykonywania operacji zbiorów (np. przecięcia, sumy) oraz usuwania duplikatów z sekwencji.

<br>
Słowniki to struktury danych, które przechowują pary klucz-wartość. Każdy klucz musi być unikalny. Słowniki są używane do szybkiego wyszukiwania wartości na podstawie klucza.

<br><br># List
numbers_list = [1, 2, 3, 4, 2]
mixed_list = [1, "hello", 3.14, True]

# Tuple
numbers_tuple = (1, 2, 3, 4, 2)
mixed_tuple = (1, "hello", 3.14, True)

# Set
numbers_set = {1, 2, 3, 4, 2}   # Przechowa jedynie unikalne wartosci {1,2,3,4} 
mixed_set = {1, "hello", 3.14, True}

# Dictionary
num_as_key = {25: "John", 30: "Jane"}
str_as_key = {"name": "Alice", "age": 28, "city": "London"}
<br><br><br># Tworzenie listy
fruits = ["apple", "banana", "cherry"]

# Dodawanie elementu na koniec
fruits.append("orange")
print(fruits)  # Wynik: ['apple', 'banana', 'cherry', 'orange']

# Wstawianie elementu na określone miejsce
fruits.insert(1, "grape")
print(fruits)  # Wynik: ['apple', 'grape', 'banana', 'cherry', 'orange']

# Usuwanie elementu po wartości
fruits.remove("banana")
print(fruits)  # Wynik: ['apple', 'grape', 'cherry', 'orange']

# Usuwanie elementu po indeksie
del fruits[2]
print(fruits)  # Wynik: ['apple', 'grape', 'orange']

# Dostęp do elementu
print(fruits[0])  # Wynik: apple

# Długość listy
print(len(fruits))  # Wynik: 3

# Sortowanie listy (rosnąco)
fruits.sort()
print(fruits)  # Wynik: ['apple', 'grape', 'orange'] (sortowanie alfabetyczne)

# Sortowanie malejąco
fruits.sort(reverse=True)
<br><br>Krotki są niemutowalne, więc nie można dodawać ani usuwać elementów po ich utworzeniu!!! Aby zrobić zmiany można skopiować wartości z już istniejącej tupli do nowej.<br># Tworzenie tuple
numbers = (1, 2, 3)

# Dostęp do elementu
print(numbers[1])  # Wynik: 2

# Długość tuple
print(len(numbers))  # Wynik: 3
<br><br># Tworzenie setu
my_set = {1, 2, 3, 2}  # Duplikaty są usuwane

# Dodawanie elementu
my_set.add(4)
print(my_set)  # Wynik: {1, 2, 3, 4}

# Usuwanie elementu
my_set.remove(2)
print(my_set)  # Wynik: {1, 3, 4}

# Długość setu
print(len(my_set))  # Wynik: 3

# Operacje na zbiorach (przecięcie, suma, różnica)
set1 = {1, 2, 3}
set2 = {2, 3, 4}
print(set1.union(set2))  # Suma zbiorów: {1, 2, 3, 4}
print(set1.intersection(set2))  # Przecięcie zbiorów: {2, 3}
<br><br># Tworzenie słownika
person = {"name": "John", "age": 30, "city": "New York"}

# Dodawanie pary klucz-wartość
person["country"] = "USA"
print(person)

# Usuwanie pary klucz-wartość
del person["age"]
print(person)

# Dostęp do wartości
print(person["name"])  # Wynik: John

# Długość słownika (liczba par klucz-wartość)
print(len(person))  # Wynik: 2

# Sprawdzenie, czy klucz istnieje
if "city" in person:
    print("Klucz 'city' istnieje")
<br><br>Przejdź do kolejnego zagadnienia (<a data-href="Pętle i instrukcje warunkowe" href="tematy\files-wstęp-do-języka-python\pętle-i-instrukcje-warunkowe.html" class="internal-link" target="_self" rel="noopener nofollow">Pętle i instrukcje warunkowe</a>) lub kliknij <a data-tooltip-position="top" aria-label="Wstęp do języka Python" data-href="Wstęp do języka Python" href="tematy\wstęp-do-języka-python.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.]]></description><link>tematy\files-wstęp-do-języka-python\listy,-krotki,-zbiory-i-słowniki.html</link><guid isPermaLink="false">Tematy/Files Wstęp do języka Python/Listy, krotki, zbiory i słowniki.md</guid><pubDate>Sat, 19 Oct 2024 23:22:12 GMT</pubDate></item><item><title><![CDATA[Pętle w języku Python]]></title><description><![CDATA[ 
 <br><br>Pętle w Pythonie służą do wielokrotnego wykonywania bloku kodu, tak długo jak dany warunek jest spełniony. Są wykorzystywane przy powtarzalnych zadaniach. Python oferuje dwa główne typy pętli: for i while.<br>W obu pętlach możliwe jest wykorzystanie dodatkowych funkcji:<br>
<br>break - przerwanie wykonania pętli,
<br>continue - przejście do następnej iteracji pętli (dalsza część obecnej iteracji się nie wykona).
<br><br>Pętla while wykonuje się tak długo, jak długo określony warunek jest prawdziwy.<br>count = 0
while count &lt; 5:
    print(count)
    count += 1
<br><br>Pętla for służy do iterowania po sekwencjach:<br># Iterowanie po zakresie liczb
for i in range(5):
    print(i)

# Iterowanie po liście
fruits = ["apple", "banana", "cherry"]
for fruit in fruits:
    print(fruit)
<br>Możliwe jest również wykorzystanie funkcji enumerate(zmienna), aby podczas iterowania po sekwencji mieć również informację o indeksie elementu:<br>fruits = ["apple", "banana", "cherry"]

for index, fruit in enumerate(fruits):
    print(f"Owoc na pozycji {index}: {fruit}")


# enumerate można też wykorzystać do utworzenia słownika
my_dict = dict(enumerate(fruits)) 
print(my_dict) # Wynik: {0: 'apple', 1: 'banana', 2: 'cherry'}
<br><br>Za pomocą pętli w Pythonie można w bardzo zwięzły i czytelny sposób tworzyć lub modyfikować listy:<br># Przykład 1
numbers = [1, 2, 3, 4, 5] 
squares = [x**2 for x in numbers] 
print(squares) # Wynik: [1, 4, 9, 16, 25]

# Przykład 2
words = ["apple", "banana", "cherry", "eagle"]
vowels = "aeiou" 
filtered_words = [word for word in words if word[0] in vowels] print(filtered_words) # Wynik: ['apple', 'eagle']

# `for word in words`: Iteruje po każdym słowie w liście `words`.
# `if word[0] in vowels`: Sprawdza, czy pierwsza litera słowa jest samogłoską.
# Jeśli warunek jest spełniony, słowo jest dodawane do nowej listy `filtered_words`
<br><br>Instrukcje warunkowe pozwalają na wykonanie określonych fragmentów kodu w zależności od spełnienia określonych warunków/kryteriów. Wyróżnić można następujące instrukcje:<br>
<br>if - podstawowy (pierwszy) warunek do sprawdzenia,
<br>elif - sprawdzenie kolejnego warunku, jeżeli poprzednie nie były spełnione,
<br>else - wykonanie w przypadku, gdy żaden wcześniejszy warunek nie został spełniony.
<br>Dostępne są następujące operatory porównania<br>
<br>== - równe,
<br>!= - różne,
<br>&lt; - mniejsze niż,
<br>&lt;= - mniejsze lub równe,
<br>&gt; - większe niż,
<br>&gt;= - większe lub równe;<br>
oraz następujące operatory logiczne
<br>and - oba warunki muszą być spełnione,
<br>or - przynajmniej jeden warunek musi być spełniony,
<br>not - negacja warunku.
<br>Przykłady:<br># Przykład 1
liczba = 10

if liczba &gt; 0:
    print("Liczba jest dodatnia.")
elif liczba &lt; 0:
    print("Liczba jest ujemna.")
else:
    print("Liczba jest równa zero.")


# Przykład 2
x = 10
y = 5

if x &gt; 5 and y &lt; 10:
	print("Oba warunki są spełnione.")
elif x &gt; 5 and not y &lt; 10:
	print("X jest wiekszy niz 5, a y nie jest wiekszy niz 10.")
<br><br>Przejdź do kolejnego zagadnienia (<a data-href="Funkcje i klasy" href="tematy\files-wstęp-do-języka-python\funkcje-i-klasy.html" class="internal-link" target="_self" rel="noopener nofollow">Funkcje i klasy</a>) lub kliknij <a data-tooltip-position="top" aria-label="Wstęp do języka Python" data-href="Wstęp do języka Python" href="tematy\wstęp-do-języka-python.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.]]></description><link>tematy\files-wstęp-do-języka-python\pętle-i-instrukcje-warunkowe.html</link><guid isPermaLink="false">Tematy/Files Wstęp do języka Python/Pętle i instrukcje warunkowe.md</guid><pubDate>Sun, 06 Oct 2024 20:42:43 GMT</pubDate></item><item><title><![CDATA[Język interpretowany]]></title><description><![CDATA[ 
 <br><br>Python jest językiem interpretowanym, więc do korzystania z niego nie jest wymagany kompilator (wykorzystywany jest interpreter). Innymi przykładami tego typu języków są Bash, PHP lub JavaScript. Oznacza to, że skrypt jest przechowywany w formie kodu i interpretowany przy każdym uruchomieniu, a nie konwertowany na kod maszynowy jednorazowo <a data-footref="interp" href="about:blank#fn-1-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a>.<br><br><img alt="PythonWhere.png" src="lib\media\pythonwhere.png"><br>
Źródło: <a data-footref="meme2" href="about:blank#fn-2-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[2]</a><br>Tak jak na obrazku powyżej, programując w Pythonie na końcu linii nie stawia się średnika. Dla tych, którym w C++ się o tym zapomina, może być to ułatwienie, ale dla osób, które pisały "kilka linijek w jednej linijce" i oddzielały je średnikami to będzie wymagało przestawienia.<br>Na obrazku powyżej wspomniane były też klamry, bo ich też nie ma (więcej o tym jak wyglądają pętle i instrukcje warunkowe będzie dalej). Ale jeśli nie ma klamr, to skąd interpreter ma wiedzieć, które linie kodu są wewnątrz if'a, a które już poza? I tutaj przechodzimy do kolejnego obrazka...<br><img alt="NoIndent.png" src="lib\media\noindent.png"><br>
Źródło: <a data-footref="meme1" href="about:blank#fn-3-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[3]</a><br><br>Programując w C++ wcięcia zwiększają jedynie czytelność kodu dla programisty i można je pomijać. W przypadku Pythona wcięcia muszą być identyczne, żeby interpreter dobrze przetworzył kod. Wydaje mi się, że najlepiej jest to pokazać na przykładzie, więc taki znajduje się poniżej.<br>for i in range(3): 
	for j in range(3):
		result = i*10+j
		print(result)
	print("Wykonam się po skończeniu się pętli z j")
print("Wykonam się po skończeniu się pętli z i")
<br>Nie wchodzimy na razie w szczegóły pętli i jak działa in range, ale wynikiem byłoby:<br>0
1
2
Wykonam się po skończeniu się pętli z j
10
11
12
Wykonam się po skończeniu się pętli z j
20
21
22
Wykonam się po skończeniu się pętli z j
Wykonam się po skończeniu się pętli z i
<br>Czyli jak widać wpierw w pętli wewnętrznej (z podwójnym wcięciem) jest dodawanie, a następnie wypisywanie w konsoli wyniku. Po wykonaniu się pętli wewnętrznej, w konsoli pojawia się odpowiednia wiadomość (zwrócę uwagę, że for j in range(3): i print("Wykonam się po skończeniu się pętli z j")również mają identyczne wcięcia, bo są wewnątrz pętli ze zmienną i). Gdy pętla zewnętrzna wykona się odpowiednią ilość razy to w konsoli również pojawia się odpowiednia wiadomość.<br>W przykładzie użyłem tabulatora do robienia wcięć, ale równie dobrze można używać spacji - jedynie wcięcie musi mieć zawsze równą wielkość!<br><br>W Pythonie komentarze dodaje się po #, np:<br># Ta linia to komentarz
print("Hello world!") # A tu komentarz w linii z kodem
<br>Możliwe są również komentarze wieloliniowe poprzez umieszczenie ich wewnątrz ``` ```. Ten drugi jest przydatny np. do przygotowania opisu funkcji, gdzie można w ten sposób opisać czym jest każda zmienna.<br><br>Przejdź do kolejnego zagadnienia (<a data-href="Zmienne i podstawowe typy danych" href="tematy\files-wstęp-do-języka-python\zmienne-i-podstawowe-typy-danych.html" class="internal-link" target="_self" rel="noopener nofollow">Zmienne i podstawowe typy danych</a>) lub kliknij <a data-tooltip-position="top" aria-label="Wstęp do języka Python" data-href="Wstęp do języka Python" href="tematy\wstęp-do-języka-python.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.<br><br><br><br><br>
<br>
<br><a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Interpreter_(computing)" target="_blank">https://en.wikipedia.org/wiki/Interpreter_(computing)</a><a href="about:blank#fnref-1-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
<br><a rel="noopener nofollow" class="external-link" href="https://programmerhumor.io/python-memes/after-using-only-c-and-c-for-years-python-felt-kinda-strange/" target="_blank">https://programmerhumor.io/python-memes/after-using-only-c-and-c-for-years-python-felt-kinda-strange/</a><a href="about:blank#fnref-2-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
<br><a rel="noopener nofollow" class="external-link" href="https://programmerhumor.io/programming-memes/no-offence-python-from-one-python-lover-to-others/" target="_blank">https://programmerhumor.io/programming-memes/no-offence-python-from-one-python-lover-to-others/</a><a href="about:blank#fnref-3-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
]]></description><link>tematy\files-wstęp-do-języka-python\składnia-w-języku-python.html</link><guid isPermaLink="false">Tematy/Files Wstęp do języka Python/Składnia w języku Python.md</guid><pubDate>Sun, 06 Oct 2024 20:42:48 GMT</pubDate><enclosure url="lib\media\pythonwhere.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pythonwhere.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Po co używać biblioteki?]]></title><description><![CDATA[ 
 <br><br>Jeżeli będziemy pracowali nad jakimś projektem związanym z ... w sumie praktycznie z czymkolwiek to jest duże prawdopodobieństwo, że ktoś już coś podobnego robił i potrzebował do tego przygotować sobie narzędzia (funkcje, klasy) i stworzył z ich wykorzystaniem bibliotekę, więc możemy ją pobrać i korzystać z tego co ktoś przygotował i zaoszczędzić przy tym czas. Wg <a data-footref="mgl" href="about:blank#fn-1-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a> istnieje ponad 137.000 bibliotek, ale spokojnie, my będziemy korzystać z kilkunastu (być może kilkudziesięciu, ale to pojedyncze funkcje będą). To jak dużo jest bibliotek dobrze jest widoczne na zrzucie ekranu poniżej.<br><img alt="Libraries_in_python.png" src="lib\media\libraries_in_python.png"><br><br>Instalowanie bibliotek odbywa się z wykorzystaniem pip. Najlepiej odszukać w internecie nazwę interesującej nas biblioteki (bo nie zawsze nazwa przy importowaniu jest jednakowa jak przy instalacji), a następnie w konsoli użyć komendy:<br>pip install nazwa_biblioteki
<br>W przypadku instalowania wielu bibliotek można to zrobić następująco:<br>pip install nazwa_biblioteki1 nazwa_biblioteki2
<br>Czasami może być konieczność zainstalowania konkretnej wersji biblioteki (np. aby była kompatybilna z inną biblioteką). Należy wówczas użyć:<br>pip install nazwa_biblioteki==wersja
<br>lub kombinacji &gt;, &lt;, &gt;= lub &lt;=, np.<br>pip install numpy&lt;=1.25.2
<br>lub<br>pip install "numpy&gt;1.18.5,&lt;=1.25.2"
<br>Dobrym zwyczajem jest również tworzenie pliku requirements.txt, w którym wypisane są wszystkie wykorzystywane biblioteki (jest to zwykły plik tekstowy, który można zrobić nawet w notatniku). Pozwoli to każdemu komu wyślemy kod na zainstalowanie tych samych bibliotek i tym samym uruchomienie skryptu (bez zainstalowania niestety nie pójdzie). Po utworzeniu pliku requirements.txt można łatwo całość zainstalować poleceniem<br>pip install -r requirements.txt
<br><br>Po zainstalowaniu biblioteki, jej zaimportowanie jest proste. Wystarczy w skrypcie wpisać<br>import nazwa_biblioteki
<br>Możliwe jest również nadanie aliasu (np. jeżeli biblioteka ma długą nazwę) lub zaimportowanie jedynie pewnych składników<br>import nazwa_biblioteki as alias_bibl # Importowanie z aliasem

from nazwa_biblioteki import funkcja, klasa # Importowanie wybranych składników
<br>Tip
Należy zwrócić uwagę na to w jaki sposób importujemy bibliotekę, bo ma to wpływ na dalszy kod (np. po nadaniu aliasu nie możemy odwołać się do biblioteki pod jej standardową nazwą)
<br>Korzystanie z funkcji jest również stosunkowo proste. Załóżmy, że w bilioteka są funkcje funkcja1 i funkcja2. Poniżej znajduje się przykład z wszystkimi przedstawionymi powyżej możliwościami importu<br>import os # Importowanie biblioteki
import numpy as np # Importowanie biblioteki z aliasem
from pandas import DataFrame, read_csv # Importowanie tylko klasy i funkcji

# Utworzenie DataFrame z biblioteki Pandas przez wczytanie danych z pliku txt
data_frame = read_csv(os.path.join("/path/to/","filename.csv"))
# Konwersja na numpy array (to_numpy() jest metodą DataFrame)
array = data_frame.to_numpy()

# Obliczenie sumy wszystkich elementów wektora
array_sum = np.sum(array)
<br><br>Kliknij <a data-tooltip-position="top" aria-label="Wstęp do języka Python" data-href="Wstęp do języka Python" href="tematy\wstęp-do-języka-python.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu. Możesz stamtąd przejść do wykonania zadań przygotowanych w Jupyter Notebook.<br><br><br>
<br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.mygreatlearning.com/blog/open-source-python-libraries/" target="_blank">https://www.mygreatlearning.com/blog/open-source-python-libraries/</a><a href="about:blank#fnref-1-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
]]></description><link>tematy\files-wstęp-do-języka-python\używanie-bibliotek.html</link><guid isPermaLink="false">Tematy/Files Wstęp do języka Python/Używanie bibliotek.md</guid><pubDate>Sun, 06 Oct 2024 20:42:52 GMT</pubDate><enclosure url="lib\media\libraries_in_python.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\libraries_in_python.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Podstawowe typy danych w języku Python]]></title><description><![CDATA[ 
 <br><br>Typ danych określa rodzaj informacji, którą możemy przechowywać w zmiennej. Python jest językiem dynamicznie typowanym, co oznacza, że nie musimy jawnie deklarować typu zmiennej przed jej użyciem. Interpreter automatycznie rozpoznaje typ na podstawie przypisanej wartości.<br><br>Przykłady tworzenia zmiennych poszczególnych typów (do przypisania służy =):<br>i = 37       # int
f = 92.13    # float
s = "Hello!" # string
b = True     # bool
<br>Do sprawdzania typu zmiennej służy funkcja type(zmienna):<br>a = 13.23
print(type(a)) # &lt;class 'float'&gt;
<br>Ze względu na dynamiczne typowanie, w Pythonie możliwe jest nadpisywanie typu danych, np:<br>x = 5.93
x = "Hello"
<br>Możliwa jest również konwersja typów przy pomocy funkcji int(zmienna), float(zmienna) lub str(zmienna):<br>num = 927.27
num_as_str = str(num)
print(num_as_str)      # "927.27"
<br><img alt="PythonDataTypes.png" src="lib\media\pythondatatypes.png"><br>
Źródło: <a data-footref="meme1" href="about:blank#fn-1-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a><br><br><br>W języku Python dostępne są następujące operatory arytmetyczne do zmiennych liczbowych.<br><br>Możliwe jest również wykorzystanie skróconych przypisań, co jest przydatne przy zmianie wartości, a służą do tego +=, -=, *=, /=, %=, **=.<br><br>Dla ciągów znaków nie można bezpośrednio zastosować tych samych operatorów arytmetycznych. Konieczna jest wpierw konwersja stringa na liczbę przy użyciu int(zmienna) lub float(zmienna). Stringi są niemutowalne, więc nie można zmienić pojedynczych znaków, a trzeba utworzyć nowy string.<br>Funkcje/operatory dostępne do wykorzystania ze stringami:<br><br><br>Przejdź do kolejnego zagadnienia (<a data-href="Listy, krotki, zbiory i słowniki" href="tematy\files-wstęp-do-języka-python\listy,-krotki,-zbiory-i-słowniki.html" class="internal-link" target="_self" rel="noopener nofollow">Listy, krotki, zbiory i słowniki</a>) lub kliknij <a data-tooltip-position="top" aria-label="Wstęp do języka Python" data-href="Wstęp do języka Python" href="tematy\wstęp-do-języka-python.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.<br><br><br>
<br>
<br><a rel="noopener nofollow" class="external-link" href="https://programmerhumor.io/python-memes/im-a-python-programmer-2/" target="_blank">https://programmerhumor.io/python-memes/im-a-python-programmer-2/</a><a href="about:blank#fnref-1-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
]]></description><link>tematy\files-wstęp-do-języka-python\zmienne-i-podstawowe-typy-danych.html</link><guid isPermaLink="false">Tematy/Files Wstęp do języka Python/Zmienne i podstawowe typy danych.md</guid><pubDate>Sun, 06 Oct 2024 20:43:09 GMT</pubDate><enclosure url="lib\media\pythondatatypes.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pythondatatypes.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Definicja i zasada działania]]></title><description><![CDATA[ 
 <br><br>Gradient Boosting iteracyjnie tworzy silny model predykcyjny, poprzez łączenie wyników wielu słabszych modeli (zazwyczaj drzew decyzyjnych), które poprawiają błędy poprzednich modeli. Każdy kolejny model jest trenowany na resztkowych błędach (residuals) poprzedniego modelu. Kluczowa jest optymalizacja funkcji straty za pomocą algorytmu gradientowego, który ma na celu minimalizację błędu między predykcją modelu a rzeczywistymi wartościami.<br><img alt="3D_Surface_with_Minima.png" src="lib\media\3d_surface_with_minima.png"><br>
Rysunek: Przykład funkcji z globalnym minimum.<br>Na obrazku powyżej, minimalizacja funkcji będzie polegała na znalezieniu takich wartości x i y, dla których wartość funkcji na osi z będzie najmniejsza (w tym kierunku zmniejsza się gradient).<br><br>Wzmocnienie gradientowe można stosować w szerokim zakresie problemów związanych z uczeniem maszynowym:<br>
<br>klasyfikacja: Gradient Boosting jest stosowany tam, gdzie liczy się precyzyjne klasyfikowanie (np. w analizach medycznych lub wykrywaniu oszustw),
<br>regresja: wzmocnienie gradientowe radzi sobie świetnie z problemami przewidywania wartości nawet w bardzo złożonych i nieliniowych modelach (np. przewidywanie cen nieruchomości czy wartości portfeli inwestycyjnych),
<br>systemy rekomendacyjne i rankingi (np. w systemach rekomendacji filmów lub innych produktów),
<br>zawody w uczeniu maszynowym: ze względu na wysoką skuteczność i precyzyjne wyniki, Gradient Boosting jest jedną z najczęściej używanych metod w zawodach Kaggle oraz w zadaniach wymagających maksymalnej dokładności.
<br><br>W porównaniu do regresji liniowej, Gradient Boosting umożliwia modelowanie bardzo złożonych danych, których zależności mogą być nieliniowe, dzięki czemu jest bardziej elastyczny. Pojedyncze drzewa decyzyjne mają skłonności do przeuczania i mogą być niedokładne, a wzmocnienie gradientowe trenuje drzewa w sekwencji, w której każde drzewo poprawia błędy poprzednika. Jest to też różnica z lasem losowym, w którym agregowane jest wiele losowo wygenerowanych drzew, ale te drzewa nie uwzględniają błędów poprzedników, więc Gradient Boosting umożliwia lepszą optymalizację.<br><br>
<br>Zalety:

<br>wysoka dokładność:  szczególnie gdy dane są złożone to Gradient Boosting często przewyższa inne metody w wielu zadaniach predykcji,
<br>elastyczność: umożliwia użycie różnych typów danych (ciągłych i kategorycznych) oraz modelowanie złożonych zależności nieliniowych,
<br>dzięki iteracyjnemu procesowi uczy się z błędów poprzednich modeli, co prowadzi do bardziej precyzyjnych predykcji.


<br>Wady:

<br>Gradient Boosting może być wolniejszy w porównaniu z innymi metodami (np. lasami losowymi), ponieważ proces jest iteracyjny i każdy model musi zostać wytrenowany jeden po drugim,
<br>jeśli model jest źle skonfigurowany (np. zbyt wiele drzew lub zbyt wysoki learning rate), może łatwo dojść do overfittingu, co skutkuje słabą generalizacją,
<br>przy uczeniu modelu ze wzmocnieniem gradientowym należy dostroić wiele parametrów (np. learning rate, liczba drzew), co może być czasochłonne i wymaga doświadczenia.


<br><br>Przejdź do kolejnego zagadnienia (<a data-href="Kluczowe pojęcia w Gradient Boosting" href="tematy\files-wzmocnienie-gradientowe-(gradient-boosting)\kluczowe-pojęcia-w-gradient-boosting.html" class="internal-link" target="_self" rel="noopener nofollow">Kluczowe pojęcia w Gradient Boosting</a>]) lub kliknij <a data-tooltip-position="top" aria-label="Wzmocnienie gradientowe (Gradient boosting)" data-href="Wzmocnienie gradientowe (Gradient boosting)" href="tematy\wzmocnienie-gradientowe-(gradient-boosting).html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.]]></description><link>tematy\files-wzmocnienie-gradientowe-(gradient-boosting)\czym-jest-gradient-boosting.html</link><guid isPermaLink="false">Tematy/Files Wzmocnienie gradientowe (Gradient boosting)/Czym jest Gradient Boosting.md</guid><pubDate>Sun, 13 Oct 2024 15:48:50 GMT</pubDate><enclosure url="lib\media\3d_surface_with_minima.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\3d_surface_with_minima.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Jakie biblioteki do wzmocnienia gradientowego?]]></title><description><![CDATA[ 
 <br><br>Gradient Boosting został zaimplementowany w 3 popularnych bibliotekach:<br>
<br>Scikit-Learn,
<br>xgboost,
<br>LightGBM.
<br>Wszystkie 3 implementacje mają podobny interfejs i umożliwiają zastosowanie do regresji i klasyfikacji, co ułatwia pracę. Występują między nimi następujące różnice:<br>
<br>Scikit-Learn udostępnia prosty w użyciu algorytm,
<br>xgboost udostępnia więcej zaawansowanych hiperparametrów i możliwości tuningu, co może prowadzić do lepszej dokładności, a także dzięki lepszej optymalizacji i rownoległości jest bardziej wydajny do dużych zbiorów danych,
<br>LightGBM jest wydajny dla dużych zestawów danych o wielu wymiarach dzięki budowania drzewa na poziomie liści (zamiast poziomu drzewa) i dzielenia danych na histogramy zamiast dzielenia danych na każdą możliwą wartość.
<br>Ze względu na lepszą optymalizację korzystniej jest korzystać z xgboost lub LightGBM, a wybór jednej z tych bibliotek zależy od preferencji oraz konkretnego przypadku datasetu.<br><br>Poniżej znajduje się kod, w którym na wygenerowanym zbiorze danych zastosowano modele Gradient Boostingu z bibliotek Scikit-Learn, xgboost i LightGBM:<br>from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression
import xgboost as xgb 
import lightgbm as lgb

# Generowanie przykładowego zbioru danych
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1)

# Podział na zbiory treningowe i testowe
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Utworzenie i trenowanie modelu oraz predykacja przy pomocy Sklearn
model_sklearn = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

model_sklearn.fit(X_train, y_train)

y_pred_sklearn = model_sklearn.predict(X_test)

# Utworzenie i trenowanie modelu oraz predykacja przy pomocy xgboost
model_xgb = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

model_xgb.fit(X_train, y_train)

y_pred_xgb = model_xgb.predict(X_test)

# Utworzenie i trenowanie modelu oraz predykacja przy pomocy LightGBM
model_lgb = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

model_lgb.fit(X_train, y_train)

y_pred_lgb = model_lgb.predict(X_test)
<br><br>Poniżej znajduje się kod, w którym na wygenerowanym zbiorze danych zastosowano modele Gradient Boostingu z bibliotek Scikit-Learn, xgboost i LightGBM:<br>from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
import xgboost as xgb
import lightgbm as lgb

# Generowanie przykładowego zbioru danych
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)

# Podział na zbiory treningowe i testowe
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Utworzenie i trenowanie modelu oraz predykacja przy pomocy Sklearn
model_sklearn = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

model_sklearn.fit(X_train, y_train)

y_pred_sklearn = model_sklearn.predict(X_test)

# Utworzenie i trenowanie modelu oraz predykacja przy pomocy xgboost
model_xgb = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

model_xgb.fit(X_train, y_train)

y_pred_xgb = model_xgb.predict(X_test)

# Utworzenie i trenowanie modelu oraz predykacja przy pomocy LightGBM
model_lgb = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) 

model_lgb.fit(X_train, y_train)

y_pred_lgb = model_lgb.predict(X_test)

<br><br>Kliknij <a data-tooltip-position="top" aria-label="Wzmocnienie gradientowe (Gradient boosting)" data-href="Wzmocnienie gradientowe (Gradient boosting)" href="tematy\wzmocnienie-gradientowe-(gradient-boosting).html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu. Możesz stamtąd przejść do wykonania zadań przygotowanych w Jupyter Notebook.]]></description><link>tematy\files-wzmocnienie-gradientowe-(gradient-boosting)\implementacja-gradient-boosting.html</link><guid isPermaLink="false">Tematy/Files Wzmocnienie gradientowe (Gradient boosting)/Implementacja Gradient Boosting.md</guid><pubDate>Sun, 13 Oct 2024 16:23:47 GMT</pubDate></item><item><title><![CDATA[Loss Function (Funkcja straty)]]></title><description><![CDATA[ 
 <br><br>Kluczowym elementem w przypadku wykorzystania wzmocnienia gradientowego jest funkcja straty. Gradient Boosting iteracyjnie minimalizuje funkcję straty poprzez poprawianie błędów poprzednich drzew i optymalizację predykcji. Przykładowymi funkcjami straty są:<br>
<br>dla regresji: Mean Squared Error (MSE) - błąd średnio-kwadratowy, który mierzy różnicę między rzeczywistymi wartościami a przewidywaniami,
<br>dla klasyfikacji: Log Loss – błąd logarytmiczny, który mierzy prawdopodobieństwo przypisania do złej klasy.
<br>Model na każdym etapie wylicza gradient funkcji straty (stąd nazwa „Gradient Boosting”), który wskazuje, jak zmienić parametry, aby zmniejszyć błąd. W skrócie, Gradient Boosting stara się znaleźć kierunek największej redukcji błędu i podąża nim, poprawiając model.<br><br>Learning rate to hiperparametr, który wpływa na to, jak duży krok model wykonuje przy aktualizacji przewidywań przy budowaniu nowego drzewa. Jest to liczba między 0 a 1 (zazwyczaj blisko 0.001), która skaluje wpływ każdego drzewa na ostateczne przewidywania. Niska wartość stałej uczenia powoduje, że model będzie uczył się wolniej, ale bardziej precyzyjnie i nie "przeoczy" minimum, co może wystąpić przy za dużej stałej uczenia. Zagrożeniem jest jednak możliwość utknięcia w lokalnym minimum, dlatego dobór stałej uczenia wymaga wyczucia i doświadczenia.<br><img alt="different_learning_rates.png" src="lib\media\different_learning_rates.png"><br>
Rysunek: Porównanie jak wygląda szukanie minimum w zależności od stałej uczenia, źródło: <a data-footref="lr" href="about:blank#fn-1-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a><br>Zatem learning rate to balans między dokładnością modelu a czasem obliczeń – zbyt wysoki może prowadzić do szybkiego przeuczenia, a zbyt niski sprawia, że model wymaga więcej iteracji, aby osiągnąć dobre wyniki.<br><br>Liczba drzew (n_estimators) w Gradient Boosting to liczba iteracji, które model wykonuje, aby poprawić swoje przewidywania. Każde drzewo, na podstawie błędów resztowych swoich poprzedników, dodaje swoje poprawki do modelu. Liczba drzew wpływa następująco na model:<br>
<br>zbyt mała liczba drzew: model może być niedouczony (underfitting), ponieważ nie ma wystarczająco dużo iteracji, aby dokładnie nauczyć się zależności w danych.
<br>zbyt duża liczba drzew: może prowadzić do przeuczenia (overfitting) i mieć małą umiejętność do generalizacji.
<br>Liczba drzew musi być dostrojona w połączeniu z learning rate - jeżeli stała uczenia jest niska to potrzebne jest więcej drzew, aby osiągnąć wysoką dokładność i uniknąć przeuczenia.<br><br>Overfitting to zjawisko, w którym model staje się zbyt dopasowany do danych treningowych, co prowadzi do gorszej wydajności na nowych, niewidzianych wcześniej danych. Gradient Boosting, choć bardzo efektywny, może być podatny na przeuczenie, zwłaszcza gdy model zawiera zbyt wiele drzew, zbyt duży learning rate lub zbyt głębokie drzewa. Aby temu zapobiec, można zastosować następujące techniki:<br>
<br>max depth (maksymalna głębokość drzewa): płytsze drzewa są mniej podatne na przeuczenie, ale wymagają większej liczby iteracji, aby uzyskać dobre wyniki.
<br>subsampling (podpróbkowanie): technika polega na używaniu tylko części danych do trenowania każdego drzewa, co w rezultacie powoduje, że model jest bardziej odporny na szumy i przeuczenie.
<br><br>Przejdź do kolejnego zagadnienia (<a data-href="Implementacja Gradient Boosting" href="tematy\files-wzmocnienie-gradientowe-(gradient-boosting)\implementacja-gradient-boosting.html" class="internal-link" target="_self" rel="noopener nofollow">Implementacja Gradient Boosting</a>]) lub kliknij <a data-tooltip-position="top" aria-label="Wzmocnienie gradientowe (Gradient boosting)" data-href="Wzmocnienie gradientowe (Gradient boosting)" href="tematy\wzmocnienie-gradientowe-(gradient-boosting).html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej tematu.<br><br><br>
<br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.jeremyjordan.me/nn-learning-rate/" target="_blank">https://www.jeremyjordan.me/nn-learning-rate/</a><a href="about:blank#fnref-1-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
]]></description><link>tematy\files-wzmocnienie-gradientowe-(gradient-boosting)\kluczowe-pojęcia-w-gradient-boosting.html</link><guid isPermaLink="false">Tematy/Files Wzmocnienie gradientowe (Gradient boosting)/Kluczowe pojęcia w Gradient Boosting.md</guid><pubDate>Sun, 13 Oct 2024 15:37:55 GMT</pubDate><enclosure url="lib\media\different_learning_rates.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\different_learning_rates.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Analiza danych przy pomocy Pandas i numpy]]></title><description><![CDATA[ 
 <br>Pandas to biblioteka w języku Python, stworzona z myślą o łatwiej i efektywnej analizie danych. Możliwości biblioteki Pandas:<br>

<br>Wczytywanie i zapisywanie danych w różnych formatach:  CSV, Excel, JSON i SQL
<br>Przekształcanie i czyszczenie danych 
<br>**Analiza statystyczna
<br>Wizualizacja danych

<br>Important
Pandas pozwala na szybkie, elastyczne i intuicyjne operacje na danych, zarówno jednowymiarowych (Series), jak i wielowymiarowych (DataFrame).
DataFrame przypomina dwuwymiarową tablicę NumPy, ale ma etykiety kolumn i wierszy, a każda kolumna może zawierać różne typy danych. Wyodrębniając pojedynczą kolumnę lub wiersz z DataFrame, otrzymujemy jednowymiarowy obiekt Series, który przypomina jednowymiarową tablicę NumPy z etykietami.
<br><img alt="series_pandas.png" src="tematy\images\series_pandas.png"><br>Wczytywanie i zapis danych:<br>Wczytywanie danych z pliku  CSV:<br>import pandas as pd 
df = pd.read_csv('plik.csv')
<br>Zapis danych jako pliku CSV:<br>df.to_csv('nowy_plik.csv', index=False)
<br>Wczytywanie danych z pliku Excel:<br>df = pd.read_excel('plik.xlsx')
<br>Wczytywanie danych z pliku JSON:<br>df = pd.read_json('plik.json')
<br>Podstawowe Operacje na DataFrame:<br><br>
<br>Wyświetlanie ostatnich 5 wierszy:
<br>print(df.tail()) 
<br>
<br>Wyświetlanie informacji o DataFrame:
<br>print(df.info()) 
<br>
<br>Wyświetlanie podstawowych statystyk opisowych:
<br>print(df.describe())
<br>
<br>Wyświetlanie liczby wierszy i kolumn Dataframe:
<br>print(df.shape)
<br>
<br>Wyświetlanie typów danych w każdej kolumnie:
<br>print(df.dtypes)
<br>
<br>Wyświetlanie kolumn:
<br>print(df.columns)
<br>Przekształcanie, czyszczenie i transformacje danych: <br> Wyświetlanie i zliczanie unikalnych wartości w kolumnie :<br>print(df.['nazwa_kolumny'].unique())

print(df.['nazwa_kolumny'].value_counts())
<br> Usuwanie wybranej kolumny:<br>df.drop(columns=['nazwa_kolumny'], inplace=True)
<br>Important
W zastosowaniach statystycznych dane niedostępne mogą być danymi, które nie istnieją, lub danymi które istnieją, ale nie zostały zaobserwowane (np. z powodu problemu wynikającego ze sposobu zbierania danych).
Podczas oczyszczania danych przed przeprowadzeniem ich analizy często warto przeprowadzić analizę samych brakujących wartości. 
<br><br>Wyświetlanie brakujących danych:<br>print(df.isnull().sum())
<br>Usuwanie brakujących wartości<br>df.dropna(inplace=True)
<br>Uzupełnianie brakujących wartości<br>df.fillna(0, inplace=True)
<br><br>Statystyka opisowa pozwala na podsumowanie zbiorów danych za pomocą miar ilościowych. Dla przykładu prostą statystyką opisową jest liczba punktów danych. Innymi popularnymi przykładami są średnie, mediany lub dominanty. Obiekty DataFrame i Series zapewniają wygodny dostęp do statystyk opisowych za pomocą metod takich jak sum, mean i count.<br><br>import pandas as pd  
  
# Dane z firmy w działach w róznych miastach 
data = {  
    "Miasto": ["Warszawa", "Kraków", "Poznań", "Wrocław", "Gdańsk", "Sopot", "Zakopane", "Łódź"],  
    "Kawa (litry)": [200.5, 300.1, 280.3, 400.4, 310.2, 270.6, 150.3, 320.9],  
    "Brak snu (godziny)": [50, 100, 90, 130, 80, 95, 40, 85],  
    "Dział": ["Produkcja", "HR", "IT", "Produkcja", "Marketing", "IT", "Produkcja", "HR"]  
}  
   
df = pd.DataFrame(data)  
opis = df.describe()  
  
# Grupowanie danych na podstawie działu  
grupa_dzial = df.groupby("Dział").agg({  
    "Kawa (litry)": ["mean", "sum"],  
    "Brak snu (godziny)": ["mean", "sum"]  
})  
  

print("=== Statystyka opisowa dla pracowników  ===\n")  
print(opis)  
  
print("\n=== Średnie spożycie kawy oraz brak snu w różnych działach ===\n")  
print(grupa_dzial)  
  
# Szukamy działu, w którym pracownicy potrzebują najwięcej kawy  
dzial_max_kawa = df.groupby("Dział")["Kawa (litry)"].sum().idxmax()  
  
print(f"\nNajwięcej kawy pije dział: {dzial_max_kawa})

1. 
		Kawa (litry)  Brak snu (godziny)
count       8.000000            8.000000
mean      279.162500           83.750000
std        71.570789           28.643055
min       150.300000           40.000000
25%       255.225000           77.500000
50%       290.250000           87.500000
75%       312.675000           95.000000
max       400.400000          130.000000


2.

				Kawa (litry)        Brak snu (godziny)     
                  mean    sum               mean  sum
Dział                                                
HR              310.50  621.0          92.500000  185
IT              275.45  550.9          92.500000  185
Marketing       310.20  310.2          80.000000   80
Produkcja       250.40  751.2          73.333333  220
<br>Filtrowanie wierszy, gdzie wartość w kolumnie 'wiek' jest większa niż 25:<br>df_filtered = df[df['wiek'] &gt; 25]
<br>Filtrowanie wierszy, gdzie wartość w kolumnie 'wiek' jest większa niż 25 i w kolumnie 'miasto' jest równa 'Poznań':<br>df_filtered = df[(df['wiek'] &gt; 25) &amp; (df['miasto'] == 'Poznań')]
<br>Grupowanie danych i obliczanie statystyk dla każdej grupy:<br>grouped = df.groupby('nazwa_kolumny').mean()
<br>Sortowanie danych według wybranej kolumny:<br>df_sorted = df.sort_values(by='nazwa_kolumny', ascending=True)
<br>Zmian nazw kolumn:<br>df.rename(columns={'stara_nazwa': 'nowa_nazwa'}, inplace=True)
<br><br>Mapowanie polega na zastąpieniu jednej wartości inną, np. w oparciu o słownik.<br><br>W przykładzie mamy dane dotyczące produktów mięsnych, a my chcemy dodać kolumnę z informacją o zwierzęciu, z którego pochodzi dany produkt.<br>import pandas as pd  
  
data = pd.DataFrame({  
    'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami', 'corned beef', 'Bacon', 'pastrami', 'honey ham', 'nova lox'],  
    'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]  
})  
  
meat_to_animal = {  
    'bacon': 'pig',  
    'pulled pork': 'pig',  
    'pastrami': 'cow',  
    'corned beef': 'cow',  
    'honey ham': 'pig',  
    'nova lox': 'salmon'  
}  
  
# Zmapowanie typu mięsa na zwierzę  
data['animal'] = data['food'].str.lower().map(meat_to_animal)  
print(data)
<br>Rezultat:<br><br>Important
Szeregi czasowe to jedna z kluczowych funkcjonalności Pandas, zwłaszcza gdy pracujemy z danymi finansowymi, meteorologicznymi czy jakimikolwiek danymi zależnymi od czasu
<br>Zacznijmy od prostego przykładu pracy z datami, które przekształcimy z łańcuchów znaków na znacznik czasu.<br><br>Moduł datetime w Pythonie pozwala na tworzenie, manipulowanie i formatowanie dat oraz czasu. Używając go, możemy wykonywać podstawowe operacje na obiektach daty i czasu.<br><br>Aby utworzyć obiekt daty w Pythonie, korzystamy z klasy datetime:<br>import pandas as pd

# Konwersja daty z tekstu na obiekt Timestamp
date = pd.to_datetime("2021-07-04")
print(date)  # Wynik: 2021-07-04 00:00:00
<br><br>Jeśli mamy daty w formacie tekstowym, możemy użyć modułu dateutil, który automatycznie rozpoznaje różne formaty dat.<br>from dateutil import parser

# Parsowanie daty z tekstu
date = parser.parse("4th of July, 2021")
print(date)  # Wynik: 2021-07-04 00:00:00
<br><br>Czasami chcesz stworzyć listę dat, np. wszystkie dni w określonym okresie. Używamy do tego funkcji pd.date_range().<br> Funkcja pd.date_range przyjmuje datę początkową, datę końcową i opcjonalny kod częstotliwości oraz zwraca regularną sekwencję dat.<br># Tworzenie zakresu dat od 1 stycznia do 7 stycznia 2023
date_range = pd.date_range('2023-01-01', '2023-01-07')
print(date_range)
<br><br>Można łatwo wygenerować daty co tydzień za pomocą funkcji pd.date_range() z argumentem freq.<br># Generowanie dat co tydzień, od 1 stycznia 2023, przez 4 tygodnie
weekly_dates = pd.date_range('2023-01-01', periods=4, freq='W')
print(weekly_dates)
<br><br>W Pandas służy do tego metoda shift, która pozwala przesunąć dane w czasie o określoną liczbę wpisów. W przypadku szeregów czasowych próbkowanych z równomierną częstotliwością metoda ta pozwala na zbadanie trendów w czasie.<br>import pandas as pd  
  
data = pd.DataFrame({  
    'sales': [300, 320, 310, 330, 340],  
    'date': pd.date_range(start='2023-01-01', periods=5, freq='D')  
}).set_index('date')  

data['previous_day_sales'] = data['sales'].shift(1)  
data['sales_increase'] = data['sales'] &gt; data['previous_day_sales']  
print(data) 

#Wynik 
#           sales  previous_day_sales  sales_increase
#date                                                 
#2023-01-01    300                 NaN           False
#2023-01-02    320               300.0            True
#2023-01-03    310               320.0           False
#2023-01-04    330               310.0            True
#2023-01-05    340               330.0            True
<br><br>Obliczanie statystyk kroczących jest kolejnym rodzajem operacji specyficznych dla szeregów czasowych, które znajdziemy w Pandas. Statystyki te można obliczyć za pomocą atrybutu rolling obiektów typu Series i DataFrame. Opcja ta pozwala na tworzenie miar statystycznych, takich jak średnia krocząca, suma krocząca czy odchylenie standardowe kroczące.<br>Important
Obliczanie statystyk na określonym "ruchomym" oknie danych polega na tym, że przekształcamy dane na podstawie bieżących wartości oraz wcześniejszych obserwacji z określonego przedziału (np. liczby dni, tygodni). 
<br>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Przykładowe dane czasowe (ceny akcji)
dates = pd.date_range(start='2023-01-01', periods=30, freq='D')
prices = np.random.randint(100, 200, size=len(dates))

data = pd.DataFrame({'price': prices}, index=dates)

# 7-dniowa średnia krocząca
data['7_day_mean'] = data['price'].rolling(window=7).mean()

# Wykres
plt.figure(figsize=(10, 6))
plt.plot(data.index, data['price'], label='Cena akcji')
plt.plot(data.index, data['7_day_mean'], label='7-dniowa średnia krocząca', linestyle='--')
plt.title("7-dniowa średnia krocząca cen akcji")
plt.xlabel("Data")
plt.ylabel("Cena akcji")
plt.legend()
plt.grid(True)
plt.show()

<br><img alt="ruchome_okna.png" src="tematy\images\ruchome_okna.png"><br><br>Praca ze strefami czasowymi to wyzwanie w analizie danych czasowych, zwłaszcza jeśli dane pochodzą z różnych lokalizacji. W tym celu można używać biblioteki pytz, która umożliwia zarządzanie strefami czasowymi.<br>
<br>Kluczowe operacje:

<br>Przekształcanie dat pomiędzy strefami czasowymi.
<br>Zarządzanie czasem lokalnym i UTC.


<br>import pandas as pd  
import pytz  
  
# Dane dotyczące transakcji
data = {'transaction_time': ['2023-01-01 10:00:00', '2023-01-01 14:00:00', '2023-01-02 09:30:00'],  
        'city': ['New York', 'London', 'Tokyo']}  
  
df = pd.DataFrame(data)  
  
# Upewnienie się ze 'transaction_time' jest w formacie datetime  
df['transaction_time'] = pd.to_datetime(df['transaction_time'])  
  
# Nowa kolumna z przypisanymi strefami czasowymi  
df['transaction_time'] = df['transaction_time'].dt.tz_localize('UTC')  # Lokalizacja dat w UTC  
  
# Dostosowanie stref czasowych dla odpowiednich miast  
df.loc[df['city'] == 'New York', 'transaction_time'] = df.loc[df['city'] == 'New York', 'transaction_time'].dt.tz_convert('America/New_York')  
df.loc[df['city'] == 'London', 'transaction_time'] = df.loc[df['city'] == 'London', 'transaction_time'].dt.tz_convert('Europe/London')  
df.loc[df['city'] == 'Tokyo', 'transaction_time'] = df.loc[df['city'] == 'Tokyo', 'transaction_time'].dt.tz_convert('Asia/Tokyo')  
  
print(df) 
#Wynik
#           transaction_time      city
#0 2023-01-01 10:00:00+00:00  New York
#1 2023-01-01 14:00:00+00:00    London
#2 2023-01-02 09:30:00+00:00     Tokyo
<br><br>Tworzenie obiektu datetime64 jest proste – wystarczy podać datę jako łańcuch znaków i określić typ danych:<br>import numpy as np
# Znacznik czasu na poziomie dnia  
date_day = np.datetime64('2021-07-04', 'D')  
print(date_day)  # Wynik: 2021-07-04  
  
# Znacznik czasu na poziomie minut  
date_minute = np.datetime64('2021-07-04 12:00', 'm')  
print(date_minute)  # Wynik: 2021-07-04T12:00  
  
# Znacznik czasu na poziomie nanosekund  
date_ns = np.datetime64('2021-07-04 12:59:59.50', 'ns')  
print(date_ns)  # Wynik: 2021-07-04T12:59:59.500000000

<br><br>NumPy pozwala na wykonywanie operacji wektorowych na dużych zbiorach dat. Możemy np. dodać dni do istniejącej daty:<br>import numpy as np

# Tworzenie daty w formacie datetime64
date_np = np.datetime64('2024-19-10')
print(date_np)  # Wynik: 2024-19-10
date_range = date_np + np.arange(12)  
print(date_range)

#wynik 
['2021-07-04' '2021-07-05' '2021-07-06' '2021-07-07' '2021-07-08'
 '2021-07-09' '2021-07-10' '2021-07-11' '2021-07-12' '2021-07-13'
 '2021-07-14' '2021-07-15']
<br>
🛑 Uwaga: Kiedy używać datetime64 z NumPy?

<br>Kiedy pracujesz z dużymi zbiorami danych czasowych.
<br>Kiedy potrzebujesz wydajności w operacjach wektorowych na datach (np. dodawanie, odejmowanie dni).
<br>Kiedy wymagana jest precyzja w nanosekundach lub chcesz skorzystać z szybkich operacji na datach.

<br><br>NumPy i datetime oferują różne kody formatu, które można używać do manipulowania wyświetlaniem dat. <br>from datetime import datetime

# Wyświetlanie w formacie 'YYYY-MM-DD HH:MM:SS'
formatted_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
print(formatted_date) # Wynik 2024-10-19 15:38:34
<br><img alt="tabela_data.png" src="tematy\images\tabela_data.png"><br>Oprócz datetime64, NumPy oferuje typ timedelta64<br>Służący do przechowywania czasów trwania lub odstępów czasu dostępny. Jest on wydajniejszym zamiennikiem natywnego typu datetime.timedelta z Pythona, bazującym na typie numpy.timedelta64. <br>import numpy as np

# Obliczanie różnicy między datami
date1 = np.datetime64('2023-01-01')
date2 = np.datetime64('2023-01-10')

difference = date2 - date1
print(difference)  # Wynik: 9 days
<br>Important
Na co pozwala biblioteka Numpy?

<br>Tworzenie i manipulowanie tablicami wielowymiarowymi:


<br>Tworzenie tablic z list, zakresów, plików 
<br>Operacje matematyczne na tablicach -
<br>Indeksowanie i wycinanie tablic
<br>Zmiana kształtu tablic 


<br>Funkcje matematyczne i statystyczne:


<br>Podstawowe operacje matematyczne (dodawanie, odejmowanie, mnożenie, dzielenie)
<br>Funkcje trygonometryczne 
<br>Funkcje statystyczne (średnia, mediana, odchylenie standardowe) 


<br>Algebra liniowa:


<br>Mnożenie macierzy 
<br>Wyznacznik macierzy
<br>Rozwiązywanie układów równań liniowych
<br>Wartości własne i wektory własne


<br>Generowanie liczb losowych z różnych rozkładów (normalny, jednostajny, itp.) 
<br>Wczytywanie i zapisywanie danych

<br>Tworzenie tablicy z samymi zerami:<br>a = np.zeros((3, 4)) # 3 wiersze, 4 kolumny
<br>Obliczanie średniej:<br>a = np.array([1, 2, 3, 4, 5])
mean = np.mean(a)
print(mean)
<br><br>Ćwiczenia dla tego tematu zostały zebrane <a data-tooltip-position="top" aria-label="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks02_Zapoznanie_z_bibliotek%C4%85_pandas.ipynb" rel="noopener nofollow" class="external-link" href="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks02_Zapoznanie_z_bibliotek%C4%85_pandas.ipynb" target="_blank">tutaj</a>.<br><br>Kliknij <a data-tooltip-position="top" aria-label="Index" data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej kursu.]]></description><link>tematy\analiza-danych-przy-pomocy-pandas-i-numpy.html</link><guid isPermaLink="false">Tematy/Analiza danych przy pomocy Pandas i numpy.md</guid><pubDate>Fri, 08 Nov 2024 19:32:11 GMT</pubDate><enclosure url="tematy\images\series_pandas.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="tematy\images\series_pandas.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Klasyfikacja]]></title><description><![CDATA[ 
 <br>W zbiorze danych występuje 70 tys. obrazów, a każdy z nich opisany jest 784 cechami. Wynika to faktu, że każdy obraz ma 28x28 pikseli, a każda cecha opisuje natężenie szarości danego piksela<br>
(przyjmuje wartość od 0 do 255)<br>import tensorflow as tf  
import matplotlib.pyplot as plt  
  
mnist = tf.keras.datasets.mnist  
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  
  
X = train_images.reshape(60000, 28*28)   
y = train_labels  
  
print("Rozmiar danych X (zbiór uczący):", X.shape)  
print("Rozmiar danych y (etykiety):", y.shape)  
  
def plot_digit(image):  
    image = image.reshape(28, 28) 
    plt.imshow(image, cmap='binary')  
    plt.axis('off')  
    plt.show()  
  
# Wyświetlenie przykładów  
for i in range(5):  
    print(f"Przykład {i + 1}: Cyfra {y[i]}")  
    plot_digit(X[i])  
  
# Podział na zbiory: uczący (60 tys.) i testowy (10 tys.)  
X_train = X[:60000]  
y_train = y[:60000]  
  
X_test = test_images.reshape(10000, 28*28)  
y_test = test_labels  
  
print("Rozmiar danych uczących X_train:", X_train.shape)  
print("Rozmiar danych testowych X_test:", X_test.shape)
<br>Przykładowe dane przedstawione są poniżej:<br>
<img alt="picture_1.png" src="tematy\images\picture_1.png"><br>
<img alt="picture_4.png" src="tematy\images\picture_4.png"><br>
<img alt="picture_9.png" src="tematy\images\picture_9.png"><br>
Rozmiar danych X (zbiór uczący): (60000, 784)<br>
Rozmiar danych y (etykiety): (60000,)
<br>W powyższym przykładzie aby sprawdzić jak wyglądają cyfry przechowywane w zestawie danych wybraliśmy wektor cech danej próbki i przekształciliśmy go w macierz o rozmiarze 28x28, którą wyświetliliśmy za pomocą funkcji imshow(). Do uzyskania mapy w skali szarości wykorzystaliśmy parametr cmap='binary'. <br>
<br>Uczenie klasyfikatora binarnego<br>
Na początku uprościmy sobie zadanie i spróbujemy identyfikować jedną cyfrę na przykład 5. Jest to przykład klasyfikatora binarnego, który ma zdolność do rozpoznawania jednej z dwóch klas: piątek i niepiątek. Zadanie to rozpoczniemy od stworzenia wektorów docelowych dla tego zadania klasyfikującego.
<br>X_test = test_images.reshape(10000, 28 * 28)    
y_test = test_labels  
  
y_train_binary = (y_train == 5).astype(np.int8)  # 1 dla cyfry 5, 0 dla innych  
y_test_binary = (y_test == 5).astype(np.int8)  
  

sgd_clf = SGDClassifier(random_state=42)  
sgd_clf.fit(X_train, y_train_binary)  
  

y_pred = sgd_clf.predict(X_test)  
  
accuracy = accuracy_score(y_test_binary, y_pred)  
print(f'Dokładność klasyfikatora: {accuracy:.2f}')  
  
print("Przykładowe predykcje (1 oznacza cyfrę 5):")  
print("Predykcje:", y_pred[:10])  
print("Prawdziwe etykiety:", y_test_binary[:10])  
  
<br>
Dokładność klasyfikatora: 0.95<br>
Przykładowe predykcje (1 oznacza cyfrę 5):<br>
Predykcje: [0 0 0 0 0 0 0 0 0 0]<br>
Prawdziwe etykiety: [0 0 0 0 0 0 0 0 1 0]
<br>W przykładzie wykorzystano klasyfikator stochastycznego spadku wzdłuż gradientu w tym celu skorzystano z klasy SGDClassifier. Klasyfikator ten sprawdza się w przypadku przetwarzania bardzo dużych zbiorów danych. Wynika to faktu, że klasyfikator ten przetwarza poszczególne przykłady uczące się niezależnie od siebie, po jednym naraz.<br>
<br>Miary wydajności:<br>
Dobrym sposobem oceny modelu jest wykorzystanie sprawdzianu krzyżowego. Zastosujemy funkcję cross_val_score() do oceny naszego modelu SGDClassifier za pomocą metody sprawdzianu krzyżowego, wygenerujemy trzy podzbiory.
<br> 🚨 Ważna Uwaga<br>Important
Kroswalidacja k-krotna oznacza rozdzielenie zestawu uczącego się na k podzbiorów ( w naszym przypadku na 3), następnie wyuczany jest model k razy. W każdej iteracji, jeden z podzbiorów jest używany jako zestaw testowy, podczas gdy pozostałe k-1 podzbiory są używane do treningu modelu.
<br>from sklearn.model_selection import cross_val_score  

cross_val_scores = cross_val_score(sgd_clf, X_train, y_train_binary, cv=3, scoring="accuracy")  
print(f'Dokładność (kroswalidacja SGD): {cross_val_scores}')
<br>
 [0.95035 0.96035 0.9604 ]
<br>Jednak ten sposób oceny nie sprawdza się tak dobrze w przypadku gdy niektóre klasy występują znacznie częściej od pozostałych tzw. klasy wypaczone.<br>Innym sposobem oceny wydajności klasyfikatora jest  macierz pomyłek.<br>
            Przewidywana
                1      0
              +----+----+
         1    | TP | FN |
Rzeczywista   +----+----+
         0    | FP | TN |
              +----+----+

<br>Macierz pomyłek pozwala zrozumieć, w jaki sposób model popełnia błędy:<br>
<br>Wysokie wartości TP wskazują, że model dobrze klasyfikuje pozytywne przypadki.
<br>Wysokie wartości TN wskazują na dobry poziom klasyfikacji negatywnych przypadków.
<br>Wysokie wartości FP wskazują, że model często myli negatywne przypadki z pozytywnymi.
<br>Wysokie wartości FN wskazują, że model często przeocza pozytywne przypadki.
<br>from sklearn.metrics import accuracy_score, confusion_matrix 
from sklearn.model_selection import cross_val_predict
import seaborn as sns

# Kroswalidacja z przewidywaniami  
y_pred_cv = cross_val_predict(sgd_clf, X_train, y_train_binary, cv=3)  
accuracy = accuracy_score(y_train_binary, y_pred_cv)  
print(f'Dokładność klasyfikatora (kroswalidacja): {accuracy:.2f}')  
  
confusion_mat = confusion_matrix(y_train_binary, y_pred_cv)  
  
# Wyświetlenie macierzy pomyłek  
plt.figure(figsize=(8, 6))  
sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])  
plt.ylabel('Rzeczywista')  
plt.xlabel('Przewidywana')  
plt.title('Macierz Pomyłek')  
plt.show()
<br>
Dokładność klasyfikatora (kroswalidacja): 0.96
<br><img alt="Macierz.png" src="tematy\images\macierz.png"><br>
🚨 Ważna Uwaga<br>Important
Każdy rząd macierzy pomyłek reprezentuje klasę rzeczywista. Natomiast kolumna stanowi przewidywaną klasę. Interpretacja 53892 stanowi prawidłowo sklasyfikowane próbki niebędące piątkami tzw. PN, 687 przykładów zostało niewłaściwe uznane za 5, tzw. FP (błąd pierwszego rodzaju).<br>
W drugim rzędzie zostały umieszczone obrazy zaklasyfikowane jako piątki.<br>
Przy czym 1891 próbek zostało nieprawidłowo sklasyfikowanych jako niebędące piątkami tzw. FN natomiast 3530 zostało prawidłowo zaklasyfikowane jako 5. 
<br>
Nasz doskonały klasyfikator uzyskiwałby wyłącznie przykłady prawdziwe pozytywne i prawdziwe negatywne () na głównej przekątnej miałby niezerowe wartości. 
<br>Jeszcze inne miary:<br>
<br>Pełność 
<br>Pełność (znana również jako czułość) jest to odsetek pozytywnych przykładów prawidłowo rozpoznane przez klasyfikator.<br>
<br>
<br>( TP \ (True Positives) - liczba rzeczywistych pozytywnych przypadków, które zostały poprawnie sklasyfikowane przez model.
<br>( FN \ (False Negatives) - liczba rzeczywistych pozytywnych przypadków, które model błędnie sklasyfikował jako negatywne.
<br>
<br>F1<br>
Jest to średnia harmoniczna precyzji z pełnością.  Jak to rozumieć?<br>
Standardowa średnia traktuje wszystkie wartości jednakowo, natomiast średnia harmoniczna nadaje większą wagę małym wartością.<br>
 Gdzie: - Precyzja (Precision) to miara, która określa, jak wiele z przewidywanych pozytywnych przypadków jest rzeczywiście pozytywnych.<br>

<br>from sklearn.metrics import precision_score, recall_score, f1_score  

precision = precision_score(y_train_binary, y_pred_cv) 
recall = recall_score(y_train_binary, y_pred_cv) 
f1 = f1_score(y_train_binary, y_pred_cv)

print(f'Precyzja: {precision:.2f}') 
print(f'Pełność: {recall:.2f}') 
print(f'F1-score: {f1:.2f}')
<br>
Precyzja: 0.84<br>
Pełność: 0.65<br>
F1-score: 0.73
<br> 🚨 Ważna Uwaga Kompromis pomiędzy precyzją a pełnością<br>Important
Przy czym F1 faworyzuje klasyfikatory mające zbliżone wartości precyzji i pełności. Nie zawsze tego chcemy, przykładowo mając na celu wyuczenie klasyfikatora w określaniu bezpiecznych filmów dla dzieci bardziej zależałoby nam na tym by klasyfikator odrzucał wiele dobrych filmów (mała wartość pełności), ale zapamiętywałby jedynie bezpieczne (duża precyzja). Niekorzystna byłaby relacja odwrotna gdzie model cechowałby się dużo większą pełnością, ale dopuszczałby kilka nieodpowiednich filmów.<br>
Jednak przy trenowaniu klasyfikatora do wykrywania złodziei z zapisu kamer prawdopodobnie nic złego by się nie stało gdyby model miał przykładowo 30% precyzji przy 99% pełności. Wówczas może i byłoby kilka fałszywych alarmów ale niemal wszyscy przestępcy zostaliby złapani.
<br><br>Do tej pory każdy przykład był przydzielany wyłącznie do jednej klasy. Jednak weźmy przykład rozpoznawania twarzy, co powinien zrobić gdy na zdjęciu jest wiele osób?<br>
Powinien przydzielić po jednej etykiecie na każdą rozpoznaną twarz. Załóżmy ,że model został wyuczony do rozpoznania grupy przyjaciół na zdjęciu: Tomka, Piotrka i Agnieszki. Po zaprezentowaniu zdjęć Tomka i Piotrka powinien zostać wygenerowany wynik [True, True, False ].<br>Tego typu system klasyfikujący zdolny do wyznaczania wielu binarnych znaczników nosi nazwę klasyfikacji wieloetykietowej. <br><br>Ćwiczenia dla tego tematu zostały zebrane <a data-tooltip-position="top" aria-label="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/" rel="noopener nofollow" class="external-link" href="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/" target="_blank">tutaj</a>.<br><br>Kliknij <a data-tooltip-position="top" aria-label="Index" data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej kursu.]]></description><link>tematy\klasyfikacja.html</link><guid isPermaLink="false">Tematy/Klasyfikacja.md</guid><pubDate>Fri, 08 Nov 2024 19:29:56 GMT</pubDate><enclosure url="tematy\images\picture_1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="tematy\images\picture_1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Metryki, analiza modeli i błędów]]></title><description><![CDATA[ 
 <br><br>
Proces ten polega na podzieleniu zbioru danych na k równych części (folds).
<br>
<br>Model jest trenowany na k-1 częściach, a testowany na jednej z pozostałych.
<br>Procedura jest powtarzana k razy, za każdym razem wybierając inny podzbiór jako zbiór testowy.
<br>Wynik końcowy to średnia ze wszystkich k iteracji.
<br>
<br>Zmniejsza ryzyko overfittingu poprzez sprawdzanie modelu na różnych partiach danych.
<br>W Scikit-learn implementuje to funkcja cross_val_score z opcją KFold do określenia liczby podziałów (k).<br><br><br>GridSearchCV to metoda doboru hiperparametrów w Scikit-learn, która testuje wszystkie kombinacje zadanych wartości hiperparametrów, aby znaleźć najlepszą.<br><br>
<br>
Zdefiniowanie siatki: Użytkownik definiuje wartości hiperparametrów. Przykład dla RandomForestClassifier:

<br>n_estimators: [100, 200, 300]
<br>max_depth: [5, 10, 15]


<br>
Przegląd kombinacji: Model trenowany jest dla każdej kombinacji hiperparametrów, a wydajność oceniana na zbiorze walidacyjnym. Używa się walidacji krzyżowej (cv).

<br>
Ocena: Wyniki dla każdej kombinacji są mierzone za pomocą zdefiniowanej metryki, np. accuracy czy F1-score.

<br>
Wybór najlepszego zestawu: Po przetestowaniu wszystkich kombinacji, zestaw hiperparametrów o najlepszym wyniku zostaje wybrany.

<br>W Scikit-learn implementowane przez GridSearchCV.<br>from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# Załadowanie danych
data = load_iris()
X, y = data.data, data.target

# Podział na dane treningowe i testowe
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Definicja hiperparametrów
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15]
}

# Model Random Forest
model = RandomForestClassifier()

# Grid Search z walidacją krzyżową (5-fold) na danych treningowych
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Najlepsze parametry
print(f"Najlepsze parametry: {grid_search.best_params_}")

# Ocena na zbiorze testowym
test_accuracy = grid_search.score(X_test, y_test)
print(f"Dokładność na zbiorze testowym: {test_accuracy:.2f}")

#Wynik
#Najlepsze parametry: {'max_depth': 10, 'n_estimators': 300}
#Dokładność na zbiorze testowym: 0.98

<br><br>RandomizedSearchCV to alternatywa dla Grid Search, która nie przeszukuje wszystkich możliwych kombinacji, a jedynie losowo wybrane próbki z przestrzeni hiperparametrów.<br><br>
<br>Zdefiniowanie zakresu: Użytkownik definiuje zakresy wartości dla hiperparametrów.
<br>Losowy wybór: Algorytm losowo wybiera kombinacje wartości do przetestowania (zamiast sprawdzać wszystkie).
<br>Ocena: Kombinacje oceniane są na zbiorze walidacyjnym, a wyniki mierzone według wybranej metryki.
<br>Wybór najlepszego zestawu: Najlepsza kombinacja zostaje wybrana do trenowania finalnego modelu.
<br><br>
<br>Szybkość: Przeszukiwanie losowe jest szybsze niż Grid Search, szczególnie przy dużej liczbie parametrów.
<br>W Scikit-learn implementowane przez RandomizedSearchCV.<br>from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from scipy.stats import randint
from sklearn.metrics import accuracy_score

# Załadowanie danych
data = load_iris()
X, y = data.data, data.target

# Podział na dane treningowe i testowe
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Definicja zakresów dla hiperparametrów
param_dist = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(5, 20)
}

# Model Random Forest
model = RandomForestClassifier()

# Random Search z walidacją krzyżową (5-fold), 10 losowych próbek
random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search.fit(X_train, y_train)

# Najlepsze parametry
print(f"Najlepsze parametry: {random_search.best_params_}")

# Ocena na zbiorze testowym
y_pred = random_search.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Dokładność na zbiorze testowym: {test_accuracy:.2f}")

<br><br><br><br>
<br>n_estimators: [100, 200, 300]
<br>max_depth: [5, 10, 15]<br>
Przeszukuje wszystkie możliwe kombinacje.
<br><br>
<br>n_estimators: od 100 do 500
<br>max_depth: od 5 do 20<br>
Losowo wybiera próbki z przestrzeni tych wartości do testowania.
<br><br><br><br> Pozwala zobaczyć, jak model klasyfikuje próbki: ile razy poprawnie przewiduje daną klasę, a ile razy pomyli się, klasyfikując próbki do innych klas.<br>Important
Elementy Confusion Matrix

<br>True Positives (TP): poprawne pozytywne predykcje.
<br>True Negatives (TN): poprawne negatywne predykcje.
<br>False Positives (FP): błędne predykcje klasy pozytywnej (fałszywe alarmy).
<br>False Negatives (FN): błędne predykcje klasy negatywnej (pominięcia).

<br><br>            Predicted Positive   Predicted Negative
Actual Pos          50                    10
Actual Neg          5                     35

- Model poprawnie przewidział 50 pozytywnych przypadków (TP) i 35 negatywnych (TN).
- 5 negatywnych przypadków zostało błędnie przewidzianych jako pozytywne (FP).
- 10 pozytywnych przypadków zostało błędnie sklasyfikowanych jako negatywne (FN).
<br>from sklearn.metrics import confusion_matrix, classification_report y_true = [0, 0, 1, 1, 0, 1, 0, 1, 1, 1] # rzeczywiste etykiety 
y_pred = [0, 1, 1, 1, 0, 0, 0, 1, 1, 1] # przewidziane etykiety 

# Macierz pomyłek 
cm = confusion_matrix(y_true, y_pred) print("Macierz pomyłek:") print(cm) 

print("\nRaport klasyfikacji:") 
print(classification_report(y_true, y_pred))


#Macierz pomyłek:
#[[3 1]
 #[1 5]]

#Raport klasyfikacji:
#             precision    recall  f1-score   support

#           0       0.75      0.75      0.75         4
#          1       0.83      0.83      0.83         6

#    accuracy                           0.80        10
#   macro avg       0.79      0.79      0.79        10
#weighted avg       0.80      0.80      0.80        10

<br><br>Ćwiczenia dla tego tematu zostały zebrane <a data-tooltip-position="top" aria-label="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks08_Metryki_Oceny_Modelu.ipynb" rel="noopener nofollow" class="external-link" href="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks08_Metryki_Oceny_Modelu.ipynb" target="_blank">tutaj</a>.<br><br>Kliknij <a data-tooltip-position="top" aria-label="Index" data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej kursu.]]></description><link>tematy\metryki,-analiza-modeli-i-błędów.html</link><guid isPermaLink="false">Tematy/Metryki, analiza modeli i błędów.md</guid><pubDate>Fri, 08 Nov 2024 19:41:57 GMT</pubDate></item><item><title><![CDATA[Przygotowanie danych i inżynieria cech]]></title><description><![CDATA[ 
 <br>Preprocessing danych to kluczowy krok w uczeniu maszynowym, który ma na celu poprawienie jakości i użyteczności danych. Dane rzeczywiste często są niekompletne, zawierają brakujące wartości, szumy, błędy lub nieprawidłowości, co negatywnie wpływa na działanie modeli (dane z tutoriali są zwykle idealnie przygotowane, a rzeczywiste zbiory danych są znacznie bardziej skomplikowane i wymagają starannej obróbki przed użyciem w modelach). Dlatego konieczne jest sprawdzenie i uzupełnienie braków, usunięcie błędów oraz ewentualne stworzenie nowych cech (feature engineering), które lepiej reprezentują ukryte zależności w danych. <br>Sam proces przygotowywania danych jest czasochłonny i wymaga kreatywności, więc w tej części naszego kursu postaramy się zobaczyć jakie są pomysły i główne działania, ale na każdym datasecie trzeba zastosować indywidualne podejście. <br><img alt="cleaning_data_meme.jpg" src="tematy\images\cleaning_data_meme.jpg"><br><br>
<br><a data-tooltip-position="top" aria-label="Co zrobić po otrzymaniu zbioru danych" data-href="Co zrobić po otrzymaniu zbioru danych" href="tematy\files-przygotowanie-danych-i-inżynieria-cech\co-zrobić-po-otrzymaniu-zbioru-danych.html" class="internal-link" target="_self" rel="noopener nofollow">Co zrobić po otrzymaniu zbioru danych?</a>
<br><a data-href="Brakujące dane" href="tematy\files-przygotowanie-danych-i-inżynieria-cech\brakujące-dane.html" class="internal-link" target="_self" rel="noopener nofollow">Brakujące dane</a>
<br><a data-href="Normalizacja i standaryzacja" href="tematy\files-przygotowanie-danych-i-inżynieria-cech\normalizacja-i-standaryzacja.html" class="internal-link" target="_self" rel="noopener nofollow">Normalizacja i standaryzacja</a>
<br><a data-href="Inżynieria cech" href="tematy\files-przygotowanie-danych-i-inżynieria-cech\inżynieria-cech.html" class="internal-link" target="_self" rel="noopener nofollow">Inżynieria cech</a>
<br><br>Ćwiczenia dla tego tematu zostały zebrane <a data-tooltip-position="top" aria-label="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks05_Przygotowanie_danych_i_in%C5%BCynieria_cech.ipynb" rel="noopener nofollow" class="external-link" href="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks05_Przygotowanie_danych_i_in%C5%BCynieria_cech.ipynb" target="_blank">tutaj</a>.<br><br>Kliknij <a data-tooltip-position="top" aria-label="Index" data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej kursu.<br>* Podczas przygotowania opisów dla tego tematu, przy części zagadnień wykorzystane zostały narzędzia generatywnej sztucznej inteligencji.]]></description><link>tematy\przygotowanie-danych-i-inżynieria-cech.html</link><guid isPermaLink="false">Tematy/Przygotowanie danych i inżynieria cech.md</guid><pubDate>Fri, 08 Nov 2024 19:27:36 GMT</pubDate><enclosure url="tematy\images\cleaning_data_meme.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="tematy\images\cleaning_data_meme.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Czym jest regresja?]]></title><description><![CDATA[ 
 <br><br>Regresja służy do modelowania zależności między zmiennymi. Pozwala ona przewidzieć wartość jednej zmiennej (zależnej) na podstawie wartości innych zmiennych (niezależnych). Modele regresji mają za zadanie zminimalizować funkcję straty, którą najczęściej jest suma kwadratów różnic, ponieważ dopasowanie modelu polega wówczas na zastosowaniu metody najmniejszych kwadratów.<br>Prostym przykładem do wytłumaczenia jest przewidywanie ceny lub zarobków, które polegają na znalezieniu matematycznego wzoru służącego do wyliczenia wartości przy znanych zmiennych niezależnych. Dla ceny mieszkania mogą to być: metraż, ilość sypialni, wielkość ogrodu, lokalizacja, rok budowy budynku; natomiast dla zarobków może być to doświadczenie zawodowe, poziom wykształcenia, wiek oraz dodatkowe umiejętności.<br><img alt="linear_regression_meme.png" src="tematy\images\linear_regression_meme.png"><br>
Źródło: <a data-footref="xk" href="about:blank#fn-1-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a><br>Z regresją można się też spotkać (często nieświadomie) podczas laboratoriów, np. z fizyki, gdy na podstawie pomiarów z eksperymentów należy wyliczyć wartość jakiegoś współczynnika (np. mając pomiar wychylenia w stopniach i napięcie z czujnika) lub gdy chcemy dodać wartości brakujących pomiarów. <br><img alt="extrapolating_meme.png" src="tematy\images\extrapolating_meme.png"><br>
Źródło: <a data-footref="xk2" href="about:blank#fn-2-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[2]</a><br><br>
<br><a data-href="Rodzaje regresji" href="tematy\files-regresja-liniowa-(i-nie-tylko)\rodzaje-regresji.html" class="internal-link" target="_self" rel="noopener nofollow">Rodzaje regresji</a>
<br><a data-tooltip-position="top" aria-label="Czym jest Scikit-learn" data-href="Czym jest Scikit-learn" href="tematy\files-regresja-liniowa-(i-nie-tylko)\czym-jest-scikit-learn.html" class="internal-link" target="_self" rel="noopener nofollow">Czym jest Scikit-learn?</a>
<br><a data-href="Implementacja regresji przy pomocy Scikit-learn" href="tematy\files-regresja-liniowa-(i-nie-tylko)\implementacja-regresji-przy-pomocy-scikit-learn.html" class="internal-link" target="_self" rel="noopener nofollow">Implementacja regresji przy pomocy Scikit-learn</a>
<br><br>Ćwiczenia dla tego tematu zostały zebrane <a data-tooltip-position="top" aria-label="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks04_Regresja_liniowa_(i_nie_tylko).ipynb" rel="noopener nofollow" class="external-link" href="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks04_Regresja_liniowa_(i_nie_tylko).ipynb" target="_blank">tutaj</a>.<br><br>Kliknij <a data-tooltip-position="top" aria-label="Index" data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej kursu.<br>* Podczas przygotowania opisów dla tego tematu, przy części zagadnień wykorzystane zostały narzędzia generatywnej sztucznej inteligencji.<br><br><br><br>
<br>
<br><a data-tooltip-position="top" aria-label="https://xkcd.com/1725" rel="noopener nofollow" class="external-link" href="https://xkcd.com/1725" target="_blank">https://xkcd.com/1725/</a><a href="about:blank#fnref-1-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
<br><a data-tooltip-position="top" aria-label="https://xkcd.com/605" rel="noopener nofollow" class="external-link" href="https://xkcd.com/605" target="_blank">https://xkcd.com/605/</a><a href="about:blank#fnref-2-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
]]></description><link>tematy\regresja-liniowa-(i-nie-tylko).html</link><guid isPermaLink="false">Tematy/Regresja liniowa (i nie tylko).md</guid><pubDate>Fri, 08 Nov 2024 19:30:48 GMT</pubDate><enclosure url="tematy\images\linear_regression_meme.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="tematy\images\linear_regression_meme.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Wizualizacja danych za pomocą bibliotek Matplotlib, Seaborn i Plotly w Pytonie]]></title><description><![CDATA[ 
 <br><img alt="datavis.png" src="tematy\images\datavis.png"><br>Omówienie najpopularniejszych bibliotek do wizualizacji danych:<br><br>Szczegółowe dokumentacje:<br>
<br>
<a data-tooltip-position="top" aria-label="https://matplotlib.org/stable/contents.html" rel="noopener nofollow" class="external-link" href="https://matplotlib.org/stable/contents.html" target="_blank">Matplotlib Documentation</a>

<br>
<a data-tooltip-position="top" aria-label="https://seaborn.pydata.org/" rel="noopener nofollow" class="external-link" href="https://seaborn.pydata.org/" target="_blank">Seaborn Documentation</a>

<br>
<a data-tooltip-position="top" aria-label="https://plotly.com/python/" rel="noopener nofollow" class="external-link" href="https://plotly.com/python/" target="_blank">Plotly Documentation</a>
Matplotlib

<br><br>SEABORN<br><br><img alt="paleta_seaborn.png" src="tematy\images\paleta_seaborn.png"><br>
<img alt="skala.png" src="tematy\images\skala.png"><br>
<img alt="wykresy_seaborn.png" src="tematy\images\wykresy_seaborn.png"><br>Generowanie zakresu danych za pomocą biblioteki matplotlib<br>import numpy as np
import matplotlib.pyplot as plt

x=np.linspace(0,2,100) # generowanie danych X,Y w zakresie od 0-2 100 punktów

<br>Przykładowy wykres przy pomocy biblioteki matplotlib:<br>import matplotlib.pyplot as plt
import pandas as pd


data = {
    'Miesiąc': ['Styczeń', 'Luty', 'Marzec', 'Kwiecień', 'Maj', 'Czerwiec', 'Lipiec', 'Sierpień', 'Wrzesień', 'Październik', 'Listopad', 'Grudzień'],
    'Przychody (tys. USD)': [9500, 9800, 10200, 10300, 11000, 11500, 12000, 12500, 11500, 11000, 10500, 9900]
}

df = pd.DataFrame(data)

plt.figure(figsize=(12, 6))
plt.plot(df['Miesiąc'], df['Przychody (tys. USD)'], marker='o', linestyle='-', color='teal')
plt.title('Miesięczne przychody spółki w 2023 roku')
plt.xlabel('Miesiąc')
plt.ylabel('Przychody (tys. USD)')
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()


<br><img alt="przychody.png" src="tematy\images\przychody.png"><br>
Przykładowy wykres słupkowy przy pomocy biblioteki seaborn:<br>import seaborn as sns 
import pandas as pd

data = { 
		'Miasto': ['Warszawa', 'Kraków', 'Gdańsk', 'Wrocław', 'Poznań', 'Szczecin', 'Olsztyn'], 
	    'Cena': [800, 750, 700, 680, 650, 620, 570, 490] 
	    }
	    
df = pd.DataFrame(data)

sns.barplot(x='Miasto', y='Cena', data=df, palette='viridis') 
plt.title('Cena mieszkań w różnych miastach') 
plt.xlabel('Miasto')
plt.ylabel('Cena mieszkania (w tys. PLN)') 
plt.show()


<br>Przykładowy wykres kołowy przy pomocy biblioteki seaborn:<br>import seaborn as sns  
import pandas as pd  
import matplotlib.pyplot as plt  
  
  
data = {  
    'Kategoria': ['Narzędzia', 'Oświetlenie', 'Farby i lakiery', 'Ogrodnictwo', 'Meble'],  
    'Udział': [30, 20, 25, 15, 10]  
}  
  
df = pd.DataFrame(data)  
  
plt.figure(figsize=(8, 8))  
plt.pie(df['Udział'], labels=df['Kategoria'], autopct='%1.1f%%', colors=sns.color_palette('Set1'))  
plt.title('Udział kategorii produktów w sprzedaży w sklepie OBI')  
plt.show()



<br><img alt="sprzedaz.png" src="tematy\images\sprzedaz.png"><br>
Przykładowe wykresy przy pomocy biblioteki Plotly:<br># wykres kołowy 

import plotly.express as px  
import pandas as pd  

data = {  
    'Producent': ['Apple', 'Samsung', 'Huawei', 'Xiaomi', 'Oppo'],  
    'Udział': [40, 30, 15, 10, 5]  
}  
  
df = pd.DataFrame(data)  
  
fig = px.pie(df, values='Udział', names='Producent', title='Udział rynkowy producentów telefonów')  
fig.update_traces(textinfo='percent+label')  
fig.show()

<br><img alt="telefon.png" src="tematy\images\telefon.png"><br># wykres słupkowy  
import plotly.graph_objects as go  
import pandas as pd  
  

data = {  
    'Kraj': ['USA', 'Chiny', 'Japonia', 'Wielka Brytania', 'Australia'],  
    'Złote': [15, 12, 10, 8, 7],  
    'Srebrne': [10, 11, 9, 8, 6],  
    'Brązowe': [10, 7, 8, 6, 6]  
}  
  
df = pd.DataFrame(data)  
fig = go.Figure()  
  

fig.add_trace(go.Bar(  
    x=df['Kraj'],  
    y=df['Złote'],  
    name='Złote',  
    marker_color='gold'  
))  
  
fig.add_trace(go.Bar(  
    x=df['Kraj'],  
    y=df['Srebrne'],  
    name='Srebrne',  
    marker_color='silver'  
))  
  
fig.add_trace(go.Bar(  
    x=df['Kraj'],  
    y=df['Brązowe'],  
    name='Brązowe',  
    marker_color='#cd7f32'  
))  
   
fig.update_layout(  
    title='Liczba medali dla Top 5 Krajów na Igrzyskach Olimpijskich w Paryżu 2024',  
    xaxis_title='Kraj',  
    yaxis_title='Liczba Medali',  
    barmode='group',    
template='plotly_white'  
)  
  
fig.show()
<br><img alt="igrzyska.png" src="tematy\images\igrzyska.png"><br>
import plotly.graph_objects as go  
import pandas as pd  
  

data = {  
    'Data': ['2024-07-01', '2024-07-15', '2024-08-01', '2024-08-15', '2024-09-01'],  
    'Manchester City': [10, 12, 15, 18, 20],  
    'Real Madryt': [8, 10, 12, 14, 17],  
    'Bayern Monachium': [9, 11, 14, 16, 19],  
    'Paris Saint-Germain': [7, 9, 13, 15, 18],  
    'Liverpool': [6, 8, 11, 13, 16]  
}  
  
df = pd.DataFrame(data)  
df['Data'] = pd.to_datetime(df['Data'])  
  
  
fig = go.Figure()  
  
for team in df.columns[1:]:  
    fig.add_trace(go.Scatter(  
        x=df['Data'],  
        y=df[team],  
        mode='lines+markers',  
        name=team,  
        text=df[team],  
        textposition='top center'  
    ))  

fig.update_layout(  
    title='Zmiana punktacji Drużyn UEFA w ciągu kilku tygodni',  
    xaxis_title='Data',  
    yaxis_title='Punkty',  
    legend_title='Drużyna',  
    template='plotly_dark',  
    xaxis=dict(  
        tickformat='%Y-%m-%d',  
        tickvals=df['Data'],  
        ticktext=[date.strftime('%b %d') for date in df['Data']]  
    )  
)  
  
fig.show()
<br><img alt="UEFA.png" src="tematy\images\uefa.png"><br><br>Ćwiczenia dla tego tematu zostały zebrane <a data-tooltip-position="top" aria-label="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks03_Analiza_Danych_z_Seaborn_Pandas_i_Plotly.ipynb" rel="noopener nofollow" class="external-link" href="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks03_Analiza_Danych_z_Seaborn_Pandas_i_Plotly.ipynb" target="_blank">tutaj</a>.<br><br>Kliknij <a data-tooltip-position="top" aria-label="Index" data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej kursu.]]></description><link>tematy\wizualizacja-danych-za-pomocą-bibliotek-matplotlib,-seaborn-i-plotly-w-pytonie.html</link><guid isPermaLink="false">Tematy/Wizualizacja danych za pomocą bibliotek Matplotlib, Seaborn i Plotly w Pytonie.md</guid><pubDate>Fri, 08 Nov 2024 19:31:44 GMT</pubDate><enclosure url="tematy\images\datavis.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="tematy\images\datavis.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Wstęp do języka Python]]></title><description><![CDATA[ 
 <br>Pierwszym tematem kursu będzie krótki wstęp do programowania w Pythonie, bo będzie to język, który będziemy wykorzystywać w zadaniach i projekcie końcowym. Konkretnie powiemy tu o wersji Python 3.<br>Python jest szeroko wykorzystywany w uczeniu maszynowym, analizie danych, przy obliczeniach matematycznych, aplikacjach webowych po stronie serwera oraz do szybkiego prototypowania. Można go wykorzystać na różnych systemach, nie wymaga kompilatora, a jego składnia jest stosunkowo prosta i przypomina język angielski <a data-footref="w3s" href="about:blank#fn-1-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a>.<br><img alt="WhoWantsToBeDataScientist.png" src="tematy\images\whowantstobedatascientist.png"><br>
Źródło: <a data-footref="dsm" href="about:blank#fn-2-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[2]</a><br>W celu uproszczenia pracy, zadania będą przygotowane w formie Jupyter Notebooks, które możecie wstawić w celu uruchomienia np. na <a data-tooltip-position="top" aria-label="https://colab.research.google.com/" rel="noopener nofollow" class="external-link" href="https://colab.research.google.com/" target="_blank">Google Colab</a> lub <a data-tooltip-position="top" aria-label="https://www.kaggle.com/" rel="noopener nofollow" class="external-link" href="https://www.kaggle.com/" target="_blank">Kaggle</a> lub uruchomić lokalnie np. przy wykorzystaniu <a data-tooltip-position="top" aria-label="https://code.visualstudio.com/" rel="noopener nofollow" class="external-link" href="https://code.visualstudio.com/" target="_blank">Visual Studio Code</a>. Jak chodzi o programowanie w Pythonie (bez Notebooków) to mogę polecić korzystanie z <a data-tooltip-position="top" aria-label="https://www.jetbrains.com/pycharm/" rel="noopener nofollow" class="external-link" href="https://www.jetbrains.com/pycharm/" target="_blank">PyCharm'a</a>.<br>
<img alt="Jupyter-logo.svg" src="tematy\images\jupyter-logo.svg" style="width: 350px; max-width: 100%;"><br>
Źródło: <a data-footref="jpt" href="about:blank#fn-3-63bd3fd7fb245f76" class="footnote-link" target="_self" rel="noopener nofollow">[3]</a><br><br>
<br><a data-href="Składnia w języku Python" href="tematy\files-wstęp-do-języka-python\składnia-w-języku-python.html" class="internal-link" target="_self" rel="noopener nofollow">Składnia w języku Python</a>
<br><a data-href="Zmienne i podstawowe typy danych" href="tematy\files-wstęp-do-języka-python\zmienne-i-podstawowe-typy-danych.html" class="internal-link" target="_self" rel="noopener nofollow">Zmienne i podstawowe typy danych</a>
<br><a data-href="Listy, krotki, zbiory i słowniki" href="tematy\files-wstęp-do-języka-python\listy,-krotki,-zbiory-i-słowniki.html" class="internal-link" target="_self" rel="noopener nofollow">Listy, krotki, zbiory i słowniki</a>
<br><a data-href="Pętle i instrukcje warunkowe" href="tematy\files-wstęp-do-języka-python\pętle-i-instrukcje-warunkowe.html" class="internal-link" target="_self" rel="noopener nofollow">Pętle i instrukcje warunkowe</a>
<br><a data-href="Funkcje i klasy" href="tematy\files-wstęp-do-języka-python\funkcje-i-klasy.html" class="internal-link" target="_self" rel="noopener nofollow">Funkcje i klasy</a>
<br><a data-href="Używanie bibliotek" href="tematy\files-wstęp-do-języka-python\używanie-bibliotek.html" class="internal-link" target="_self" rel="noopener nofollow">Używanie bibliotek</a>
<br>Oczywiście nie są to wszystkie zagadnienia związane z Pythonem. Dostępnych jest wiele kursów, poradników i stron z informacjami i ćwiczeniami na ten temat. Przykładami mogą być <a data-tooltip-position="top" aria-label="https://www.w3schools.com/python/default.asp" rel="noopener nofollow" class="external-link" href="https://www.w3schools.com/python/default.asp" target="_blank">w3schools</a> i <a data-tooltip-position="top" aria-label="https://www.kaggle.com/learn/python" rel="noopener nofollow" class="external-link" href="https://www.kaggle.com/learn/python" target="_blank">kaggle</a>.<br><br><a data-tooltip-position="top" aria-label="Jak korzystać z Jupyter Notebooks" data-href="Jak korzystać z Jupyter Notebooks" href="tematy\files-wstęp-do-języka-python\jak-korzystać-z-jupyter-notebooks.html" class="internal-link" target="_self" rel="noopener nofollow">Krótki poradnik jak korzystać z Jupyter Notebooks</a><br>Ćwiczenia dla tego tematu zostały zebrane <a data-tooltip-position="top" aria-label="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks01_Wst%C4%99p_do_j%C4%99zyka_Python.ipynb" rel="noopener nofollow" class="external-link" href="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks01_Wst%C4%99p_do_j%C4%99zyka_Python.ipynb" target="_blank">tutaj</a>.<br><br>Kliknij <a data-tooltip-position="top" aria-label="Index" data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej kursu.<br>* Podczas przygotowania opisów dla tego tematu, przy części zagadnień wykorzystane zostały narzędzia generatywnej sztucznej inteligencji.<br><br><br><br><br>
<br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.w3schools.com/python/python_intro.asp" target="_blank">https://www.w3schools.com/python/python_intro.asp</a><a href="about:blank#fnref-1-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
<br><a rel="noopener nofollow" class="external-link" href="https://programmerhumor.io/python-memes/python-10/" target="_blank">https://programmerhumor.io/python-memes/python-10/</a><a href="about:blank#fnref-2-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
<br><a rel="noopener nofollow" class="external-link" href="https://www.jetbrains.com/pycharm/" target="_blank">https://www.jetbrains.com/pycharm/</a><a href="about:blank#fnref-3-63bd3fd7fb245f76" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
]]></description><link>tematy\wstęp-do-języka-python.html</link><guid isPermaLink="false">Tematy/Wstęp do języka Python.md</guid><pubDate>Fri, 08 Nov 2024 19:30:38 GMT</pubDate><enclosure url="tematy\images\whowantstobedatascientist.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="tematy\images\whowantstobedatascientist.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Iteracyjne poprawianie jakości modelu]]></title><description><![CDATA[ 
 <br><br>Gradient Boosting buduje model iteracyjnie, ucząc się na błędach poprzednich drzew i wzmacniając te obszary, gdzie inne modele były słabe, co daje bardziej precyzyjne i stabilne prognozy. Takie podejście ma przewagę nad regresją liniową, bo umożliwia modelowanie nawet bardzo złożonych zależności, oraz nad pojedynczymi drzewami decyzyjnymi, które są podatne na przeuczenie i często nie zapewniają optymalnych wyników.<br><img alt="gradient_boosting_meme.jpg" src="tematy\images\gradient_boosting_meme.jpg"><br><br>
<br><a data-tooltip-position="top" aria-label="Czym jest Gradient Boosting" data-href="Czym jest Gradient Boosting" href="tematy\files-wzmocnienie-gradientowe-(gradient-boosting)\czym-jest-gradient-boosting.html" class="internal-link" target="_self" rel="noopener nofollow">Czym jest Gradient Boosting?</a>
<br><a data-href="Kluczowe pojęcia w Gradient Boosting" href="tematy\files-wzmocnienie-gradientowe-(gradient-boosting)\kluczowe-pojęcia-w-gradient-boosting.html" class="internal-link" target="_self" rel="noopener nofollow">Kluczowe pojęcia w Gradient Boosting</a>
<br><a data-href="Implementacja Gradient Boosting" href="tematy\files-wzmocnienie-gradientowe-(gradient-boosting)\implementacja-gradient-boosting.html" class="internal-link" target="_self" rel="noopener nofollow">Implementacja Gradient Boosting</a>
<br><br>Ćwiczenia dla tego tematu zostały zebrane <a data-tooltip-position="top" aria-label="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks07_Wzmocnienie_gradientowe_(Gradient_Boosting).ipynb" rel="noopener nofollow" class="external-link" href="https://github.com/KoloNaukowe-RAI/Kurs-Machine-Learning/blob/main/Tasks/Tasks07_Wzmocnienie_gradientowe_(Gradient_Boosting).ipynb" target="_blank">tutaj</a>.<br><br>Kliknij <a data-tooltip-position="top" aria-label="Index" data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow">tutaj</a>, aby wrócić do strony głównej kursu.<br>* Podczas przygotowania opisów dla tego tematu, przy części zagadnień wykorzystane zostały narzędzia generatywnej sztucznej inteligencji.]]></description><link>tematy\wzmocnienie-gradientowe-(gradient-boosting).html</link><guid isPermaLink="false">Tematy/Wzmocnienie gradientowe (Gradient boosting).md</guid><pubDate>Fri, 08 Nov 2024 19:30:42 GMT</pubDate><enclosure url="tematy\images\gradient_boosting_meme.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="tematy\images\gradient_boosting_meme.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Cześć!]]></title><description><![CDATA[ 
 <br><br>Witaj na kursie wprowadzającym do uczenia maszynowego. Wybierz interesujący Cię temat z listy poniżej:<br><br><br><a data-tooltip-position="top" aria-label="https://github.com/dariak153" rel="noopener nofollow" class="external-link" href="https://github.com/dariak153" target="_blank">@dariak153</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/mmcza" rel="noopener nofollow" class="external-link" href="https://github.com/mmcza" target="_blank">@mmcza</a>]]></description><link>index.html</link><guid isPermaLink="false">Index.md</guid><pubDate>Sat, 26 Oct 2024 15:55:11 GMT</pubDate></item></channel></rss>